{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1858411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a81d190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d920a65",
   "metadata": {},
   "source": [
    "### NLP Processing\n",
    "\n",
    "Reference: https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "edf53b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description):\n",
    "    # Function to convert a raw webpage description to a string of words\n",
    "    # The input is a single string (webpage description), and \n",
    "    # the output is a single string (a preprocessed webpage description)\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    description_text = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", description_text) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Stem the words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in meaningful_words]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space and return the result.\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "    \n",
    "def nlp_preprocessing(webpage_description, vectorizer_name='tfidf', max_words_in_vocab=None):\n",
    "    \n",
    "    print(\"Cleaning webpage description...\")\n",
    "    # Preprocess each description in the column according to the function described above\n",
    "    cleaned_webpage_description = webpage_description.progress_apply(lambda x: preprocess_webpage_description(x))\n",
    "    \n",
    "    # Initialize vectorizer according to input parameters\n",
    "    if vectorizer_name == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_words_in_vocab)\n",
    "    elif vectorizer_name == \"count\":\n",
    "        vectorizer = CountVectorizer(max_features=max_words_in_vocab)\n",
    "\n",
    "    print(\"Applying vectorizer...\")\n",
    "    # Apply vectorizer to the data\n",
    "    vectorized_webpage_description = vectorizer.fit_transform(cleaned_webpage_description)\n",
    "    \n",
    "    # Converting data to a DataFrame so that it can be processed later more easily\n",
    "    vectorized_webpage_description = pd.DataFrame(vectorized_webpage_description.toarray())\n",
    "    print(\"Finished vectorization\")\n",
    "    \n",
    "    return vectorized_webpage_description\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b172e29",
   "metadata": {},
   "source": [
    "### General Preprocessing\n",
    "\n",
    "- Does data format fixing\n",
    "- Fills in missing values\n",
    "- Generates websiteName feature\n",
    "- Drops redundant or invalid columns\n",
    "- Does NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "39b1d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset_input, max_words_in_vocab=None):\n",
    "    \n",
    "    # Columns to drop\n",
    "    # framebased because its all 0s\n",
    "    # url because after generating websiteName feature we can drop it\n",
    "    # others because they are highly correlated with other features in the dataset\n",
    "    features_to_drop = ['framebased', 'embedRatio', 'AvglinkWithTwoCommonWord', 'AvglinkWithThreeCommonWord', 'url']\n",
    "    \n",
    "    # Doing a copy so that the input dataset remains intact\n",
    "    dataset = dataset_input.copy(deep=True)\n",
    "    \n",
    "    # Convert webpageDescription from string to JSON\n",
    "    dataset['webpageDescription'] = dataset['webpageDescription'].apply(lambda x: json.loads(x))\n",
    "    \n",
    "    # Replace webpageDescription by its \"body\" content, if there's no \"body\" content, then replace by \"title\" content\n",
    "    dataset['webpageDescription'] = dataset['webpageDescription'].apply(lambda x: x['title'] if x['body'] == None else x['body'])\n",
    "\n",
    "    # Generate the websiteName feature\n",
    "    dataset['websiteName'] = dataset['url'].apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "\n",
    "    # Only retain those website_names with atleast 30 entries, assign all other website names to 'other' general category\n",
    "    website_names = dataset['websiteName'].value_counts()\n",
    "    websitesWithAtleast30Entries = list(website_names[website_names > 30].index)\n",
    "    dataset['websiteName'] = dataset['websiteName'].apply(lambda x: x if x in websitesWithAtleast30Entries else 'other')\n",
    "\n",
    "    # Drop the following columns,\n",
    "    dataset.drop(features_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # Vectorize the webpageDescription data\n",
    "    # Specify the name of vectorizer as \"tfidf\" or \"count\" for CountVectorizer\n",
    "    # Can also pass in the maximum words to be retained in vocabulary, otherwise vectorizer will consider all the words in the vocabulary\n",
    "    # max_words_in_vocab=5000 will consider the 5000 most frequently occurring words in the dataset\n",
    "    vectorized_data = nlp_preprocessing(dataset['webpageDescription'], vectorizer_name='tfidf', max_words_in_vocab=max_words_in_vocab)\n",
    "        \n",
    "    processed_data = pd.concat([dataset, vectorized_data], axis=1)\n",
    "    \n",
    "    # CODE THAT REPLACES THE ? VALUES\n",
    "    \n",
    "    # Replace all ? values in isNews and isFrontPageNews by new category 'unknown'\n",
    "    processed_data['isNews'] = processed_data['isNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    processed_data['isFrontPageNews'] = processed_data['isFrontPageNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "    # Assign all ? values in alchemy_category to \"unknown\" category\n",
    "    processed_data['alchemy_category'] = processed_data['alchemy_category'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "    # For all ? alchemy_category values we assigned them to \"unknown\" category\n",
    "    # and we are 100% confident of this assignment\n",
    "    # So we substitute alchemy_category_score = 1.0 (100%) for all ? values which correspond to 'unknown' category\n",
    "    processed_data['alchemy_category_score'] = processed_data['alchemy_category_score'].apply(lambda x: 1.0 if x == '?' else float(x))\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3ddf3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:27<00:00, 267.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "processed_data = preprocessing(merged_data, max_words_in_vocab=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfe8ac",
   "metadata": {},
   "source": [
    "### Train test split for regular training\n",
    "\n",
    "This function does the following,\n",
    "- Takes in the combined processed dataset as input\n",
    "- Applies get_dummies on the categorical columns\n",
    "- Removes webpageDescription & id from the data because they are not required for training\n",
    "- Applies train_test_split with test_size = 0.3\n",
    "- Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "- Returns X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22b0ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_training(dataset, random_state=42):\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    cur_dataset = train_data.copy(deep=True)\n",
    "    \n",
    "    numerical_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "\n",
    "    cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "\n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    X = cur_dataset.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y = cur_dataset['label']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Standard Scaler requires us to save the changes in a copy instead of the original dataframe so that's why these copies are made\n",
    "    X_train_copy = X_train.copy(deep=True)\n",
    "    X_test_copy = X_test.copy(deep=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # Feature Standardization\n",
    "    for feature in numerical_features:\n",
    "        scaler.fit(X_train_copy[[feature]])\n",
    "        X_train_copy[feature] = scaler.transform(X_train_copy[[feature]])\n",
    "        X_test_copy[feature] = scaler.transform(X_test_copy[[feature]])\n",
    "    \n",
    "    return X_train_copy, X_test_copy, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07341cc1",
   "metadata": {},
   "source": [
    "### Train-test Split for final submission\n",
    "\n",
    "Very similar to the above function with the few changes being,\n",
    "- There is no actual train_test_split() call here as we use the full train.csv data\n",
    "- Apply get_dummies and feature standardization on the entire data (train.csv + test.csv)\n",
    "- Separates out train.csv and test.csv data from this processed data\n",
    "- Returns X_train (that has been processed from train.csv), y (from train.csv) & X_test (that has been processed from test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "605a4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_final_submission(dataset):    \n",
    "    cur_dataset = dataset.copy(deep=True)\n",
    "    \n",
    "    numerical_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "\n",
    "    cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "\n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Feature Standardization\n",
    "    for feature in numerical_features:\n",
    "        cur_dataset[feature] = scaler.fit_transform(cur_dataset[[feature]])\n",
    "    \n",
    "    train_data = cur_dataset[cur_dataset['label'].isna() == False]\n",
    "    test_data = cur_dataset[cur_dataset['label'].isna() == True]\n",
    "    \n",
    "    X_train = train_data.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    # Do not drop \"id\" from X_test\n",
    "    X_test = test_data.drop(['label', 'webpageDescription'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551aef80",
   "metadata": {},
   "source": [
    "### Example use-case of preparing_data_for_training() for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db0dad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eb531b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796701978279803\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33640a4",
   "metadata": {},
   "source": [
    "### Example use-case of creating final submission using preparing_data_for_final_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7322e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, y_train_final, test = preparing_data_for_final_submission(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0f8acb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Drop 'id' before sending for training\n",
    "y_final_pred = model.predict(test.drop('id', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3a03bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing file to be submitted\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df[\"id\"] = test[\"id\"]\n",
    "submission_df[\"label\"] = y_final_pred\n",
    "submission_df.to_csv(\"Submission_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
