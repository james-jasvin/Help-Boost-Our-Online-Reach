{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68c20de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Logging to display info regarding training of models especially Word2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e8b4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Convert webpageDescription from string to JSON\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d6fc511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def use_body_key(x):\n",
    "    if x['body'] == None or len(x['body'].strip()) == 0:\n",
    "        if x['title'] == None or len(x['title'].strip()) == 0:\n",
    "            if x['url'] == None or len(x['url'].strip()) == 0:\n",
    "                return 'unknown'\n",
    "            return x['url']\n",
    "        return x['title']\n",
    "    \n",
    "    return x['body']\n",
    "\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_body_key(x))\n",
    "print(merged_data['webpageDescription'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c74f0",
   "metadata": {},
   "source": [
    "word2vec requires a single sentence as input and a sentence is treated as a list of words, so this function returns a list of words\n",
    "\n",
    "Removing stopwords and numbers can be detrimental to the learning process, so they're not removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cacadc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description, remove_stopwords=False, no_empty_lists=False):\n",
    "    # Function to convert a raw webpage description to a string of words\n",
    "    # The input is a single string (webpage description), and \n",
    "    # the output is a single string (a preprocessed webpage description)\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    words = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-alphanumeric values\n",
    "    words = re.sub(\"[^a-zA-Z\\d]\", \" \", words) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = words.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025c55b",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.\n",
    "\n",
    "It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8f9a9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c5f915a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a description into parsed sentences\n",
    "def description_to_sentences(description, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a description into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(description.strip())\n",
    "    \n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call preprocess_webpage_description to get a list of words\n",
    "            sentences.append(preprocess_webpage_description(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796e199",
   "metadata": {},
   "source": [
    "Apply this function on the entire dataset to convert each description into a list of descriptions, i.e. list of list of sentences, where all the sentences are split into words but all of the words are combined into one list which is why we use += in the below for loop instead append() method\n",
    "\n",
    "For example,\n",
    "\n",
    "    A = [[1,2],[2,3]]\n",
    "    B = [[3,4],[4,5]]\n",
    "    A.append(B) = [[1,2],[2,3],[[3,4],[4,5]]] => Not what we want\n",
    "    A += B => A = [[1,2],[2,3],[3,4],[4,5]] => What we want\n",
    "\n",
    "This only happens when you're trying to join 2D lists or even higher dimensional lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acf6533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██▉                                   | 566/7394 [00:00<00:04, 1431.08it/s]/home/jasvin/.local/lib/python3.8/site-packages/bs4/__init__.py:337: MarkupResemblesLocatorWarning: \".\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 27%|█████████▉                           | 1993/7394 [00:01<00:03, 1410.53it/s]/home/jasvin/.local/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.youtube.com/user/Jeepersmedia.\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 43%|███████████████▋                     | 3144/7394 [00:02<00:02, 1451.48it/s]/home/jasvin/.local/lib/python3.8/site-packages/bs4/__init__.py:337: MarkupResemblesLocatorWarning: \"/\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 7394/7394 [00:05<00:00, 1404.70it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "for desc in tqdm(merged_data['webpageDescription']):\n",
    "    sentences += description_to_sentences(desc, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "801869c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polyvore', 'is', 'the', 'best', 'place', 'to', 'discover', 'or', 'start', 'fashion', 'trends']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6908b",
   "metadata": {},
   "source": [
    "### Training Word2Vec model\n",
    "\n",
    "References for understanding the various parameters of the model:\n",
    "\n",
    "- https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
    "- https://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec708e03",
   "metadata": {},
   "source": [
    "\n",
    "    Architecture: Architecture options are skip-gram (default) or continuous bag of words. skip-gram\n",
    "    typically produces better results.\n",
    "    FIND OUT WHAT WORKS BEST\n",
    "    \n",
    "    Training algorithm: Hierarchical softmax (default) or negative sampling\n",
    "    FIND OUT WHAT WORKS BEST\n",
    "    \n",
    "    Downsampling of frequent words: The Google documentation recommends values between .00001 and .001\n",
    "    FIND OUT WHAT WORKS BEST\n",
    "    \n",
    "    Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better \n",
    "    models. Reasonable values can be in the tens to hundreds; we used 300\n",
    "    \n",
    "    Context / window size: How many words of context should the training algorithm take into account? 10 seems to \n",
    "    work well for hierarchical softmax (more is better, up to a point).\n",
    "    \n",
    "    Worker threads: Number of parallel processes to run. \n",
    "    This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "    \n",
    "    Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not \n",
    "    occur at least this many times across all documents is ignored. \n",
    "    Reasonable values could be between 10 and 100. \n",
    "\n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "96ebc2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 01:19:09,352 : INFO : collecting all words and their counts\n",
      "2021-12-10 01:19:09,354 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-10 01:19:09,874 : INFO : PROGRESS: at sentence #10000, processed 2109284 words, keeping 65616 word types\n",
      "2021-12-10 01:19:10,148 : INFO : collected 85239 word types from a corpus of 3349470 raw words and 15767 sentences\n",
      "2021-12-10 01:19:10,150 : INFO : Creating a fresh vocabulary\n",
      "2021-12-10 01:19:10,504 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 85239 unique words (100.0%% of original 85239, drops 0)', 'datetime': '2021-12-10T01:19:10.504407', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2021-12-10 01:19:10,505 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3349470 word corpus (100.0%% of original 3349470, drops 0)', 'datetime': '2021-12-10T01:19:10.505677', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2021-12-10 01:19:11,103 : INFO : deleting the raw counts dictionary of 85239 items\n",
      "2021-12-10 01:19:11,106 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2021-12-10 01:19:11,107 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2761360.9470575512 word corpus (82.4%% of prior 3349470)', 'datetime': '2021-12-10T01:19:11.107869', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2021-12-10 01:19:12,231 : INFO : estimated required memory for 85239 words and 300 dimensions: 247193100 bytes\n",
      "2021-12-10 01:19:12,232 : INFO : resetting layer weights\n",
      "2021-12-10 01:19:12,369 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-10T01:19:12.369683', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'build_vocab'}\n",
      "2021-12-10 01:19:12,370 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 85239 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2021-12-10T01:19:12.370699', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'train'}\n",
      "2021-12-10 01:19:13,514 : INFO : EPOCH 1 - PROGRESS: at 3.65% examples, 88922 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:14,528 : INFO : EPOCH 1 - PROGRESS: at 7.36% examples, 94606 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:15,636 : INFO : EPOCH 1 - PROGRESS: at 11.69% examples, 98099 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:16,755 : INFO : EPOCH 1 - PROGRESS: at 15.81% examples, 101881 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:17,804 : INFO : EPOCH 1 - PROGRESS: at 19.88% examples, 104429 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:18,860 : INFO : EPOCH 1 - PROGRESS: at 23.59% examples, 103338 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:19,904 : INFO : EPOCH 1 - PROGRESS: at 28.21% examples, 105597 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:20,915 : INFO : EPOCH 1 - PROGRESS: at 32.33% examples, 107890 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:21,916 : INFO : EPOCH 1 - PROGRESS: at 38.80% examples, 112295 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:22,916 : INFO : EPOCH 1 - PROGRESS: at 43.93% examples, 113845 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:23,920 : INFO : EPOCH 1 - PROGRESS: at 49.36% examples, 115683 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:24,930 : INFO : EPOCH 1 - PROGRESS: at 54.63% examples, 118398 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:25,980 : INFO : EPOCH 1 - PROGRESS: at 59.24% examples, 119343 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:27,055 : INFO : EPOCH 1 - PROGRESS: at 64.44% examples, 120731 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:28,060 : INFO : EPOCH 1 - PROGRESS: at 70.22% examples, 122382 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:29,104 : INFO : EPOCH 1 - PROGRESS: at 75.06% examples, 123384 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:30,132 : INFO : EPOCH 1 - PROGRESS: at 79.82% examples, 124180 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:31,230 : INFO : EPOCH 1 - PROGRESS: at 84.96% examples, 124707 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:32,240 : INFO : EPOCH 1 - PROGRESS: at 89.96% examples, 125811 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:33,270 : INFO : EPOCH 1 - PROGRESS: at 94.44% examples, 125755 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:34,172 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-12-10 01:19:34,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 01:19:34,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 01:19:34,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 01:19:34,230 : INFO : EPOCH - 1 : training on 3349470 raw words (2760888 effective words) took 21.9s, 126351 effective words/s\n",
      "2021-12-10 01:19:35,235 : INFO : EPOCH 2 - PROGRESS: at 4.70% examples, 127368 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:36,261 : INFO : EPOCH 2 - PROGRESS: at 9.55% examples, 130462 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:37,380 : INFO : EPOCH 2 - PROGRESS: at 14.71% examples, 131546 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:38,507 : INFO : EPOCH 2 - PROGRESS: at 20.18% examples, 134283 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:39,586 : INFO : EPOCH 2 - PROGRESS: at 26.04% examples, 136460 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:40,588 : INFO : EPOCH 2 - PROGRESS: at 31.07% examples, 138785 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:41,663 : INFO : EPOCH 2 - PROGRESS: at 36.58% examples, 137606 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:42,707 : INFO : EPOCH 2 - PROGRESS: at 42.29% examples, 136077 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:43,718 : INFO : EPOCH 2 - PROGRESS: at 47.49% examples, 136519 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:44,722 : INFO : EPOCH 2 - PROGRESS: at 52.84% examples, 137232 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:45,763 : INFO : EPOCH 2 - PROGRESS: at 57.36% examples, 135335 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:46,836 : INFO : EPOCH 2 - PROGRESS: at 62.32% examples, 135471 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:47,845 : INFO : EPOCH 2 - PROGRESS: at 67.58% examples, 135917 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:48,876 : INFO : EPOCH 2 - PROGRESS: at 72.38% examples, 135609 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:49,940 : INFO : EPOCH 2 - PROGRESS: at 76.82% examples, 134918 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:51,000 : INFO : EPOCH 2 - PROGRESS: at 81.20% examples, 134233 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:52,131 : INFO : EPOCH 2 - PROGRESS: at 86.47% examples, 134464 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:53,207 : INFO : EPOCH 2 - PROGRESS: at 91.68% examples, 134750 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:19:54,237 : INFO : EPOCH 2 - PROGRESS: at 97.30% examples, 135208 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:54,487 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-12-10 01:19:54,531 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 01:19:54,532 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 01:19:54,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 01:19:54,559 : INFO : EPOCH - 2 : training on 3349470 raw words (2761458 effective words) took 20.3s, 135854 effective words/s\n",
      "2021-12-10 01:19:55,601 : INFO : EPOCH 3 - PROGRESS: at 4.54% examples, 119841 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:56,615 : INFO : EPOCH 3 - PROGRESS: at 9.04% examples, 122436 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 01:19:57,651 : INFO : EPOCH 3 - PROGRESS: at 14.49% examples, 129059 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:58,731 : INFO : EPOCH 3 - PROGRESS: at 18.74% examples, 128165 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:19:59,792 : INFO : EPOCH 3 - PROGRESS: at 24.06% examples, 131163 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:00,798 : INFO : EPOCH 3 - PROGRESS: at 29.23% examples, 132657 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:01,878 : INFO : EPOCH 3 - PROGRESS: at 34.63% examples, 133405 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:02,910 : INFO : EPOCH 3 - PROGRESS: at 40.92% examples, 134660 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:03,971 : INFO : EPOCH 3 - PROGRESS: at 46.75% examples, 135124 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:04,997 : INFO : EPOCH 3 - PROGRESS: at 51.47% examples, 134455 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:06,003 : INFO : EPOCH 3 - PROGRESS: at 57.12% examples, 135752 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:07,035 : INFO : EPOCH 3 - PROGRESS: at 61.96% examples, 136455 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:08,080 : INFO : EPOCH 3 - PROGRESS: at 67.10% examples, 135816 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:09,124 : INFO : EPOCH 3 - PROGRESS: at 72.23% examples, 135858 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:10,204 : INFO : EPOCH 3 - PROGRESS: at 76.82% examples, 135514 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:11,276 : INFO : EPOCH 3 - PROGRESS: at 81.20% examples, 134688 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:12,344 : INFO : EPOCH 3 - PROGRESS: at 86.29% examples, 134919 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:13,378 : INFO : EPOCH 3 - PROGRESS: at 90.94% examples, 134643 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:14,458 : INFO : EPOCH 3 - PROGRESS: at 96.23% examples, 134457 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:14,963 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-12-10 01:20:14,967 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 01:20:15,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 01:20:15,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 01:20:15,020 : INFO : EPOCH - 3 : training on 3349470 raw words (2761259 effective words) took 20.5s, 135004 effective words/s\n",
      "2021-12-10 01:20:16,054 : INFO : EPOCH 4 - PROGRESS: at 4.70% examples, 124308 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:17,107 : INFO : EPOCH 4 - PROGRESS: at 9.35% examples, 123286 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:18,167 : INFO : EPOCH 4 - PROGRESS: at 14.54% examples, 129315 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:19,175 : INFO : EPOCH 4 - PROGRESS: at 19.03% examples, 130625 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:20,189 : INFO : EPOCH 4 - PROGRESS: at 24.33% examples, 134306 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:21,191 : INFO : EPOCH 4 - PROGRESS: at 29.23% examples, 134071 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:22,213 : INFO : EPOCH 4 - PROGRESS: at 34.63% examples, 135747 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:23,238 : INFO : EPOCH 4 - PROGRESS: at 40.72% examples, 135913 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:24,280 : INFO : EPOCH 4 - PROGRESS: at 46.56% examples, 136475 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:25,333 : INFO : EPOCH 4 - PROGRESS: at 51.47% examples, 136052 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:26,353 : INFO : EPOCH 4 - PROGRESS: at 57.12% examples, 137049 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:27,399 : INFO : EPOCH 4 - PROGRESS: at 61.96% examples, 137492 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:28,423 : INFO : EPOCH 4 - PROGRESS: at 67.43% examples, 137470 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:29,436 : INFO : EPOCH 4 - PROGRESS: at 72.38% examples, 137777 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:30,500 : INFO : EPOCH 4 - PROGRESS: at 77.15% examples, 137445 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:31,587 : INFO : EPOCH 4 - PROGRESS: at 82.16% examples, 137627 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:32,601 : INFO : EPOCH 4 - PROGRESS: at 87.28% examples, 138292 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:33,615 : INFO : EPOCH 4 - PROGRESS: at 92.00% examples, 137985 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:34,668 : INFO : EPOCH 4 - PROGRESS: at 97.75% examples, 138099 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:34,894 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-12-10 01:20:34,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 01:20:34,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 01:20:34,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 01:20:34,951 : INFO : EPOCH - 4 : training on 3349470 raw words (2761292 effective words) took 19.9s, 138586 effective words/s\n",
      "2021-12-10 01:20:36,012 : INFO : EPOCH 5 - PROGRESS: at 4.87% examples, 129039 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:37,020 : INFO : EPOCH 5 - PROGRESS: at 9.91% examples, 134966 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:38,040 : INFO : EPOCH 5 - PROGRESS: at 14.71% examples, 134286 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:39,153 : INFO : EPOCH 5 - PROGRESS: at 19.85% examples, 135016 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:40,185 : INFO : EPOCH 5 - PROGRESS: at 24.87% examples, 135541 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:41,312 : INFO : EPOCH 5 - PROGRESS: at 30.47% examples, 136322 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:42,332 : INFO : EPOCH 5 - PROGRESS: at 36.34% examples, 137550 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:43,362 : INFO : EPOCH 5 - PROGRESS: at 42.29% examples, 137213 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:44,413 : INFO : EPOCH 5 - PROGRESS: at 47.69% examples, 137789 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:45,428 : INFO : EPOCH 5 - PROGRESS: at 52.67% examples, 136667 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:46,452 : INFO : EPOCH 5 - PROGRESS: at 57.92% examples, 137037 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:47,495 : INFO : EPOCH 5 - PROGRESS: at 61.57% examples, 134701 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:48,504 : INFO : EPOCH 5 - PROGRESS: at 65.95% examples, 133157 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:49,511 : INFO : EPOCH 5 - PROGRESS: at 71.10% examples, 133391 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:50,524 : INFO : EPOCH 5 - PROGRESS: at 75.30% examples, 133037 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:51,559 : INFO : EPOCH 5 - PROGRESS: at 79.55% examples, 132360 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:52,562 : INFO : EPOCH 5 - PROGRESS: at 83.42% examples, 131738 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:53,628 : INFO : EPOCH 5 - PROGRESS: at 87.50% examples, 130552 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:54,665 : INFO : EPOCH 5 - PROGRESS: at 91.68% examples, 129711 words/s, in_qsize 7, out_qsize 0\n",
      "2021-12-10 01:20:55,666 : INFO : EPOCH 5 - PROGRESS: at 96.51% examples, 129531 words/s, in_qsize 8, out_qsize 0\n",
      "2021-12-10 01:20:56,114 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-12-10 01:20:56,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 01:20:56,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 01:20:56,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 01:20:56,173 : INFO : EPOCH - 5 : training on 3349470 raw words (2761402 effective words) took 21.2s, 130134 effective words/s\n",
      "2021-12-10 01:20:56,174 : INFO : Word2Vec lifecycle event {'msg': 'training on 16747350 raw words (13806299 effective words) took 103.8s, 133005 effective words/s', 'datetime': '2021-12-10T01:20:56.174731', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 01:20:56,175 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=85239, vector_size=300, alpha=0.025)', 'datetime': '2021-12-10T01:20:56.175768', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "2021-12-10 01:20:56,177 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_1minwords_10context_sg', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-12-10T01:20:56.177084', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'saving'}\n",
      "2021-12-10 01:20:56,178 : INFO : storing np array 'vectors' to 300features_1minwords_10context_sg.wv.vectors.npy\n",
      "2021-12-10 01:20:56,246 : INFO : storing np array 'syn1neg' to 300features_1minwords_10context_sg.syn1neg.npy\n",
      "2021-12-10 01:20:56,295 : INFO : not storing attribute cum_table\n",
      "2021-12-10 01:20:56,326 : INFO : saved 300features_1minwords_10context_sg\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = Word2Vec(sentences, window=context, workers=num_workers, vector_size=num_features,\n",
    "                     min_count=min_word_count, sample=downsampling, sg=1)\n",
    "\n",
    "model_name = \"300features_1minwords_10context_sg\"\n",
    "w2v_model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31cf70",
   "metadata": {},
   "source": [
    "### Using Trained Word2Vec model to convert input descriptions into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b693e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 01:21:02,257 : INFO : loading Word2Vec object from 300features_1minwords_10context_sg\n",
      "2021-12-10 01:21:02,295 : INFO : loading wv recursively from 300features_1minwords_10context_sg.wv.* with mmap=None\n",
      "2021-12-10 01:21:02,296 : INFO : loading vectors from 300features_1minwords_10context_sg.wv.vectors.npy with mmap=None\n",
      "2021-12-10 01:21:02,361 : INFO : loading syn1neg from 300features_1minwords_10context_sg.syn1neg.npy with mmap=None\n",
      "2021-12-10 01:21:02,423 : INFO : setting ignored attribute cum_table to None\n",
      "2021-12-10 01:21:03,452 : INFO : Word2Vec lifecycle event {'fname': '300features_1minwords_10context_sg', 'datetime': '2021-12-10T01:21:03.452557', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load('300features_1minwords_10context_sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53654a",
   "metadata": {},
   "source": [
    "### Tokenize webpageDescription\n",
    "\n",
    "So that its word vectors can be individually accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e8f7fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7394/7394 [00:02<00:00, 2923.90it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_data = merged_data.copy(deep=True)\n",
    "processed_data['tokenizedDescription'] = processed_data['webpageDescription'].progress_apply(lambda x: preprocess_webpage_description(x, no_empty_lists=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231c20",
   "metadata": {},
   "source": [
    "### Convert word vectors into feature vector by averaging technique\n",
    "\n",
    "For a given webpage description, its feature vector is the average of word vectors of all the words in that description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71c7f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    # This array will contain the sum of all word vectors for the given description\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    \n",
    "    # This counts the number of words from given description whose word vectors are used\n",
    "    # to compute the overall word embedding for this description\n",
    "    nwords = 0.\n",
    "\n",
    "    # index_to_key is a list that contains the names of the words in the model's vocabulary\n",
    "    # Convert it to a set, for speed \n",
    "    index2word_set = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "    # There are outlier cases where after doing the preprocessing steps, i.e. after removing non-alphanumeric\n",
    "    # characters there are no words left, so sentence remains an empty list which is a problem for training the\n",
    "    # word2vec model, so we just return a list containing 'unknown' as the sole word\n",
    "    # Note: This typically happens with a few entries where description contains only Japanese characters and such\n",
    "    if len(words) == 0:\n",
    "        words = ['unknown']\n",
    "    \n",
    "    # Loop over each word in the description and if it is in the model's vocabulary,\n",
    "    # add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            # Add the word vector of given word in featureVec\n",
    "            featureVec = np.add(featureVec, model.wv[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    \n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(descriptions, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "     \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    descriptionFeatureVecs = np.zeros((len(descriptions),num_features),dtype=\"float32\")\n",
    "    \n",
    "    # Loop through the reviews\n",
    "    for i, description in enumerate(tqdm(descriptions)):\n",
    "         \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        descriptionFeatureVecs[i] = makeFeatureVec(description, model, num_features)\n",
    "        \n",
    "    return descriptionFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bad1832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:47<00:00, 155.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# num_features = Same as that used for training\n",
    "vectorized_data = pd.DataFrame(getAvgFeatureVecs(processed_data['tokenizedDescription'], w2v_model, num_features=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f30f5",
   "metadata": {},
   "source": [
    "### Concatenate label and id with vectorized data\n",
    "\n",
    "So that predictions can be made with labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "77bf12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_data = pd.concat([processed_data[['label','id']], vectorized_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4beb2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_training(dataset, random_state=42):\n",
    "    '''\n",
    "        Takes in the dataset as input which is the output of the preprocessing() function call\n",
    "        Applies get_dummies on the categorical columns\n",
    "        Removes webpageDescription & id from the data because they are not required for training\n",
    "        Applies train_test_split with test_size = 0.3\n",
    "        Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "        \n",
    "        Returns\n",
    "        -----------------------------\n",
    "        X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    X = train_data.drop(['label', 'id'], axis=1)\n",
    "    y = train_data['label']\n",
    "        \n",
    "    return train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "def preparing_data_for_final_submission(dataset):        \n",
    "    '''\n",
    "        Apply get_dummies and feature standardization on the entire data (train.csv + test.csv)\n",
    "        Separates out train.csv and test.csv data from this processed data\n",
    "        Returns X_train (that has been processed from train.csv), y_train (from train.csv) & X_test (that has been processed from test.csv)\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        X_train, y_train, X_test\n",
    "    '''\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    test_data = dataset[dataset['label'].isna() == True]\n",
    "    \n",
    "    X_train = train_data.drop(['label', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    \n",
    "    # Do not drop \"id\" from X_test\n",
    "    X_test = test_data.drop(['label'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def generate_csv_submission(test, y_final_pred, output_file_name='submission.csv'):\n",
    "    '''\n",
    "        Parameters\n",
    "        -----------------------\n",
    "        test: Test data that contains id column\n",
    "        \n",
    "        y_final_pred: predict_proba() output for given model and test data\n",
    "        \n",
    "        output_file_name: Name of submission output file\n",
    "    '''\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"id\"] = test[\"id\"]\n",
    "    submission_df[\"label\"] = y_final_pred\n",
    "    submission_df.to_csv(output_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3ce4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(modelling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b3d63a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8647814792525272\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4023440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(modelling_data)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = lr_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'word2vec_lr_300features_1minwords_10context_sg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac269d",
   "metadata": {},
   "source": [
    "### Approach using Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9713a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7394/7394 [00:02<00:00, 3020.53it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_data = merged_data.copy(deep=True)\n",
    "tokenized_description_data = processed_data['webpageDescription'].progress_apply(lambda x: preprocess_webpage_description(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bff20db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 19:57:49,108 : INFO : collecting all words and their counts\n",
      "2021-12-09 19:57:49,112 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-12-09 19:57:54,246 : INFO : collected 1123711 token types (unigram + bigrams) from a corpus of 3344265 words and 7394 sentences\n",
      "2021-12-09 19:57:54,247 : INFO : merged Phrases<1123711 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-12-09 19:57:54,248 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1123711 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 5.14s', 'datetime': '2021-12-09T19:57:54.248289', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigrams = Phrases(sentences=tokenized_description_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f1bffb5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 19:58:41,451 : INFO : collecting all words and their counts\n",
      "2021-12-09 19:58:41,455 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-12-09 19:58:52,612 : INFO : collected 1274399 token types (unigram + bigrams) from a corpus of 2983858 words and 7394 sentences\n",
      "2021-12-09 19:58:52,613 : INFO : merged Phrases<1274399 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-12-09 19:58:52,614 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1274399 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 11.16s', 'datetime': '2021-12-09T19:58:52.613996', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-38-generic-x86_64-with-glibc2.29', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "trigrams = Phrases(sentences=bigrams[tokenized_description_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dddf13ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [polyvore, is, the, best, place, to, discover,...\n",
       "1       [speed, air, man, david, belle, david, belle, ...\n",
       "2       [chicken, gruyere, one, of, our, favorite, spe...\n",
       "3       [oh, me, oh, my, this, was, really, snackalici...\n",
       "4       [barbecued, chicken, chow, siew, from, the, ex...\n",
       "                              ...                        \n",
       "7389                                                   []\n",
       "7390    [save, to, your, collections, sorry, for, the,...\n",
       "7391    [best, belly, dance, workout, 2011, swerve, st...\n",
       "7392    [outerbanxchic, posted, 10, 14, 2011, deliciou...\n",
       "7393    [summer, always, signifies, the, beginning, of...\n",
       "Name: webpageDescription, Length: 7394, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[tokenized_description_data].corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
