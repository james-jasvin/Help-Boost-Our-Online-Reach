{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c20de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e8b4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Convert webpageDescription from string to JSON\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fc511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def use_title_key(x):\n",
    "    # Some entries don't have title key, in that case add 'title' key with value as None\n",
    "    # to avoid KeyError in the next if condition\n",
    "    x.setdefault('title', None)\n",
    "    \n",
    "    if x['title'] == None:\n",
    "        return x['body']\n",
    "    \n",
    "    return x['title']\n",
    "\n",
    "def use_body_key(x):\n",
    "    if x['body'] == None:\n",
    "        return x['title']\n",
    "    \n",
    "    return x['body']\n",
    "\n",
    "# COMMENT OR UNCOMMENT THESE LINES DEPENDING ON WHICH DATA YOU WANT IN THE webpageDescription column\n",
    "# merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_title_key(x))\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_body_key(x))\n",
    "print(merged_data['webpageDescription'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c74f0",
   "metadata": {},
   "source": [
    "word2vec requires a single sentence as input and a sentence is treated as a list of words, so this function returns a list of words\n",
    "\n",
    "Removing stopwords and numbers can be detrimental to the learning process, so they're not removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cacadc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description, remove_stopwords=False):\n",
    "    # Function to convert a raw webpage description to a string of words\n",
    "    # The input is a single string (webpage description), and \n",
    "    # the output is a single string (a preprocessed webpage description)\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    words = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-alphanumeric values\n",
    "    words = re.sub(\"[^a-zA-Z\\d]\", \" \", words) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = words.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025c55b",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.\n",
    "\n",
    "It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9a9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f915a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a description into parsed sentences\n",
    "def description_to_sentences(description, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a description into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(description.strip())\n",
    "    \n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call preprocess_webpage_description to get a list of words\n",
    "            sentences.append(preprocess_webpage_description(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1273eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
