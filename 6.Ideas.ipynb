{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fde27704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB # Doesn't work for Word2Vec because of negative values in word vectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Logging to display info regarding training of models especially Word2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce181f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Convert webpageDescription from string to JSON\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff38f1",
   "metadata": {},
   "source": [
    "### Filling in webpageDescription\n",
    "\n",
    "    Use the body key value if non-empty\n",
    "    Else use the title key vallue\n",
    "    Else use the url key value\n",
    "    Else just fill it with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fc511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def use_body_key(x):\n",
    "    # strip() function is used to ensure that only blank descriptions don't pass through this condition\n",
    "    if x['body'] == None or len(x['body'].strip()) == 0:\n",
    "        if x['title'] == None or len(x['title'].strip()) == 0:\n",
    "            if x['url'] == None or len(x['url'].strip()) == 0:\n",
    "                return 'unknown'\n",
    "            return x['url']\n",
    "        return x['title']\n",
    "    \n",
    "    return x['body']\n",
    "\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_body_key(x))\n",
    "print(merged_data['webpageDescription'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454277f",
   "metadata": {},
   "source": [
    "### 1. Using URL column to generate websiteName feature\n",
    "\n",
    "Here is the logic used,\n",
    "    \n",
    "    - Extract the domain name out of the URL and create a new categorical column called “websiteName”.\n",
    "    - There are in total 3372 unique website names in around 7000 entries.\n",
    "    - We have 19 unique website names that have count > 30 in the dataset, so we can let these website names be \n",
    "      as it is and combine all the other website names into \"other\" category\n",
    "    - So in total there will be 20 categories in total in websiteName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf95c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the websiteName feature\n",
    "def generate_website_name(urls):\n",
    "    websites = urls.apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "\n",
    "    # Only retain those website_names with atleast 30 entries, assign all other website names to 'other' general category\n",
    "    websites_counts = websites.value_counts()\n",
    "    websites_with_atleast_30 = list(websites_counts[websites_counts > 30].index)\n",
    "    websites = websites.apply(lambda x: x if x in websites_with_atleast_30 else 'other')\n",
    "\n",
    "    return websites\n",
    "\n",
    "merged_data['websiteName'] = generate_website_name(merged_data['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638c79c",
   "metadata": {},
   "source": [
    "These are the 20 unique categories in the websiteName feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f14f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                        6258\n",
       "www.insidershealth.com        143\n",
       "sportsillustrated.cnn.com     109\n",
       "www.huffingtonpost.com         99\n",
       "allrecipes.com                 93\n",
       "bleacherreport.com             86\n",
       "www.youtube.com                85\n",
       "blogs.babble.com               62\n",
       "www.ivillage.com               59\n",
       "www.foodnetwork.com            57\n",
       "www.dailymail.co.uk            46\n",
       "www.epicurious.com             36\n",
       "www.womansday.com              35\n",
       "www.bbc.co.uk                  34\n",
       "www.popsci.com                 33\n",
       "www.guardian.co.uk             33\n",
       "www.marthastewart.com          33\n",
       "www.buzzfeed.com               31\n",
       "itechfuture.com                31\n",
       "www.collegehumor.com           31\n",
       "Name: websiteName, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['websiteName'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e85e55",
   "metadata": {},
   "source": [
    "### 2. Using Chi-Squared test to only retain the useful words in the vocabulary\n",
    "\n",
    "The Chi-Squared test checks whether each feature given in the dataset is independent of the target variable or not by considering each feature individually. But the feature must take non-negative values which in our case is perfectly fine as we're considering TF-IDF values which are always positive.\n",
    "\n",
    "It computes the p-value for each feature which tells you how effective that feature is (individually) to predict the target variable.\n",
    "\n",
    "It takes the Null Hypothesis which assumes that the feature and target variable are independent.\n",
    "\n",
    "Lower the p-value, the better the feature is.\n",
    "\n",
    "On the basis of this, the p-score is defined as, 1 - p_value\n",
    "\n",
    "A p_score of 0.95 is typically considered to be a good indicator of a feature being useful for prediction process.\n",
    "\n",
    "p-value less than 0.05 (typically ≤ 0.05) is statistically significant. It indicates strong evidence against the null hypothesis, as there is less than a 5% probability the null is correct (and the results are random). Therefore, we reject the null hypothesis, and accept the alternative hypothesis.\n",
    "\n",
    "In our task, we have around 80,000 words in the entire dataset and we prune it further to around 10-40 thousand using the max_features parameter of the vectorizer object to pick the most frequent words.\n",
    "\n",
    "But we can prune this even further with the help of this test and only choosing those words which have p-score >= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34587a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description, lemmatize=False):\n",
    "    '''\n",
    "        Function to convert a raw webpage description to a string of words\n",
    "        The input is a single string (webpage description), and \n",
    "        the output is a single string (a preprocessed webpage description)\n",
    "    '''\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    words = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-letters        \n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", words) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = words.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Stem or Lemmatize the words\n",
    "    if lemmatize == False:\n",
    "        porter = PorterStemmer()\n",
    "        words = [porter.stem(word) for word in words]\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space and return the result.\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def generate_vectorized_data(data_input, vectorizer_name='tfidf', lemmatize=False, max_words_in_vocab=None, vocabulary=None):\n",
    "    '''\n",
    "        Takes in dataset input, uses webpageDescription column applies NLP preprocessing on it\n",
    "        Then gives it to the specified vectorizer and returns vectorized data.\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        data_input: Dataframe of dataset\n",
    "        \n",
    "        vectorizer_name: Can be 'tfidf' or 'count'\n",
    "        \n",
    "        lemmatize: True => Data should be lemmatized, False => Data should be stemmed\n",
    "        \n",
    "        max_words_in_vocab: Value affects max_features parameter of vectorizer used, if None => all words are used\n",
    "        \n",
    "        vocabulary: Custom vocabulary that will be given to the vectorizer as input, if None => vocabulary is determined by the vectorizer\n",
    "\n",
    "        Returns\n",
    "        ----------------\n",
    "        vectorized_data: Dataframe of vectorized data\n",
    "    '''\n",
    "    data = data_input.copy(deep=True)\n",
    "    \n",
    "    print(\"Cleaning webpage description...\")\n",
    "    # Preprocess each description in the column according to the function described above\n",
    "    data['webpageDescription'] = data['webpageDescription'].progress_apply(lambda x: preprocess_webpage_description(x, lemmatize))\n",
    "    \n",
    "    # Initialize vectorizer according to input parameters\n",
    "    if vectorizer_name == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_words_in_vocab, vocabulary=vocabulary)\n",
    "    elif vectorizer_name == \"count\":\n",
    "        vectorizer = CountVectorizer(max_features=max_words_in_vocab)\n",
    "\n",
    "    print(\"Applying vectorizer...\")\n",
    "    \n",
    "    train_data = data[data['label'].isna() == False]\n",
    "    test_data = data[data['label'].isna() == True]\n",
    "\n",
    "    # Apply vectorizer to the data\n",
    "    # Fit vectorizer on the train data and then transform the test data (avoids data leakages)\n",
    "    vectorized_train_data = vectorizer.fit_transform(train_data['webpageDescription']).toarray()\n",
    "    vectorized_test_data = vectorizer.transform(test_data['webpageDescription']).toarray()\n",
    "        \n",
    "    vectorized_webpage_description = np.concatenate((vectorized_train_data, vectorized_test_data))\n",
    " \n",
    "    # Converting data to a DataFrame so that it can be processed later more easily\n",
    "    vectorized_webpage_description = pd.DataFrame(vectorized_webpage_description)\n",
    "    print(\"Finished vectorization\")\n",
    "    \n",
    "    return vectorized_webpage_description, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1da29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_training(dataset, random_state=42):\n",
    "    '''\n",
    "        Takes in the dataset as input which is the output of the preprocessing() function call\n",
    "        Applies get_dummies on the categorical columns\n",
    "        Removes webpageDescription & id from the data because they are not required for training\n",
    "        Applies train_test_split with test_size = 0.3\n",
    "        Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "        \n",
    "        Returns\n",
    "        -----------------------------\n",
    "        X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    X = train_data.drop(['label', 'id'], axis=1)\n",
    "    y = train_data['label']\n",
    "        \n",
    "    return train_test_split(X, y, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc6111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:30<00:00, 245.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "vectorized_data, vectorizer = generate_vectorized_data(merged_data, lemmatize=False, max_words_in_vocab=10000, vectorizer_name='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be050df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.concat([merged_data[['label','id']], vectorized_data], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=69)\n",
    "\n",
    "vocab_words = vectorizer.get_feature_names()\n",
    "\n",
    "# Min value of p_score required is p_limit\n",
    "p_limit = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19158e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>recip</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>cup</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>bake</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>butter</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>cook</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9922</th>\n",
       "      <td>yakuza</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9946</th>\n",
       "      <td>yoder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>zetaclear</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>zhao</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>zumbo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature  score\n",
       "7196      recip    1.0\n",
       "2156        cup    1.0\n",
       "634        bake    1.0\n",
       "1208     butter    1.0\n",
       "1952       cook    1.0\n",
       "...         ...    ...\n",
       "9922     yakuza    NaN\n",
       "9946      yoder    NaN\n",
       "9976  zetaclear    NaN\n",
       "9977       zhao    NaN\n",
       "9999      zumbo    NaN\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the Chi-Squared test and storing the results in a DataFrame and sorting DF by p_score value\n",
    "\n",
    "chi2_stats, p = chi2(X_train, y_train)\n",
    "\n",
    "p_score_per_word = pd.DataFrame({\"feature\":vocab_words, \"score\":1-p})\n",
    "\n",
    "# Sorting in descending order of p-score value\n",
    "p_score_per_word.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c31e0a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>add</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>android</td>\n",
       "      <td>0.962849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>app</td>\n",
       "      <td>0.970821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>asid</td>\n",
       "      <td>0.985957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>athlet</td>\n",
       "      <td>0.970253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>wrap</td>\n",
       "      <td>0.967185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>yeast</td>\n",
       "      <td>0.976111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>yogurt</td>\n",
       "      <td>0.973375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.978169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>0.991504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature     score\n",
       "87         add  1.000000\n",
       "322    android  0.962849\n",
       "386        app  0.970821\n",
       "481       asid  0.985957\n",
       "523     athlet  0.970253\n",
       "...        ...       ...\n",
       "9894      wrap  0.967185\n",
       "9935     yeast  0.976111\n",
       "9949    yogurt  0.973375\n",
       "9974      zest  0.978169\n",
       "9995  zucchini  0.991504\n",
       "\n",
       "[307 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_score_per_word_filtered = p_score_per_word[p_score_per_word['score'] > p_limit]\n",
    "\n",
    "p_score_per_word_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2da26",
   "metadata": {},
   "source": [
    "    Filtering by p_score value >= 0.95 resulted in only 305 words remaining, we'll create a new feature matrix\n",
    "    by using only these words as our input vocabulary.\n",
    "    \n",
    "    The new vectorized data was then used to train a Logistic Regression model with base parameters and this \n",
    "    actually gave us a score of 0.86302 which is the same as our base Logistic Regression model with 10,000 \n",
    "    words in the vocabulary.\n",
    "    \n",
    "    So even though the accuracy didn’t improve, there was a significant improvement in terms of memory used, \n",
    "    time required for training and scalability as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81844958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
