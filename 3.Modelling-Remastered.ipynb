{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d6ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22843064",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0093686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description, lemmatize=False):\n",
    "    '''\n",
    "        Function to convert a raw webpage description to a string of words\n",
    "        The input is a single string (webpage description), and \n",
    "        the output is a single string (a preprocessed webpage description)\n",
    "    '''\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    words = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-letters        \n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", words) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = words.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Stem or Lemmatize the words\n",
    "    if lemmatize == False:\n",
    "        porter = PorterStemmer()\n",
    "        words = [porter.stem(word) for word in words]\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space and return the result.\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def generate_vectorized_data(data_input, vectorizer_name='tfidf', lemmatize=False, max_words_in_vocab=None, vocabulary=None):\n",
    "    '''\n",
    "        Takes in dataset input, uses webpageDescription column applies NLP preprocessing on it\n",
    "        Then gives it to the specified vectorizer and returns vectorized data.\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        data_input: Dataframe of dataset\n",
    "        \n",
    "        vectorizer_name: Can be 'tfidf' or 'count'\n",
    "        \n",
    "        lemmatize: True => Data should be lemmatized, False => Data should be stemmed\n",
    "        \n",
    "        max_words_in_vocab: Value affects max_features parameter of vectorizer used, if None => all words are used\n",
    "        \n",
    "        vocabulary: Custom vocabulary that will be given to the vectorizer as input, if None => vocabulary is determined by the vectorizer\n",
    "\n",
    "        Returns\n",
    "        ----------------\n",
    "        vectorized_data: Dataframe of vectorized data\n",
    "        vectorizer: Vectorizer that was used to fit the training data on\n",
    "    '''\n",
    "    data = data_input.copy(deep=True)\n",
    "    \n",
    "    print(\"Cleaning webpage description...\")\n",
    "    # Preprocess each description in the column according to the function described above\n",
    "    data['webpageDescription'] = data['webpageDescription'].progress_apply(lambda x: preprocess_webpage_description(x, lemmatize))\n",
    "    \n",
    "    # Initialize vectorizer according to input parameters\n",
    "    if vectorizer_name == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_words_in_vocab, vocabulary=vocabulary)\n",
    "    elif vectorizer_name == \"count\":\n",
    "        vectorizer = CountVectorizer(max_features=max_words_in_vocab)\n",
    "\n",
    "    print(\"Applying vectorizer...\")\n",
    "    \n",
    "    train_data = data[data['label'].isna() == False]\n",
    "    test_data = data[data['label'].isna() == True]\n",
    "\n",
    "    # Apply vectorizer to the data\n",
    "    # Fit vectorizer on the train data and then transform the test data (avoids data leakages)\n",
    "    vectorized_train_data = vectorizer.fit_transform(train_data['webpageDescription']).toarray()\n",
    "    vectorized_test_data = vectorizer.transform(test_data['webpageDescription']).toarray()\n",
    "        \n",
    "    vectorized_webpage_description = np.concatenate((vectorized_train_data, vectorized_test_data))\n",
    " \n",
    "    # Converting data to a DataFrame so that it can be processed later more easily\n",
    "    vectorized_webpage_description = pd.DataFrame(vectorized_webpage_description)\n",
    "    print(\"Finished vectorization\")\n",
    "    \n",
    "    return vectorized_webpage_description, vectorizer\n",
    "\n",
    "def preprocessing(dataset_input, vectorized_data, features_to_use=[]):\n",
    "    '''\n",
    "        Takes in dataset and the vectorized data, concatenates them\n",
    "        It then takes a susbet of the features specified in 'features_to_use' parameter list\n",
    "    \n",
    "        Parameters\n",
    "        --------------------\n",
    "        dataset_input: Original input data\n",
    "        \n",
    "        vectorized_data: webpage description data that is output of generate_vectorized_data()\n",
    "        \n",
    "        features_to_use: Which features to use for training, features_to_use = [] => Use all features\n",
    "    \n",
    "        Returns\n",
    "        --------------------\n",
    "        processed_data: Concatenated dataframe of original data and vectorized_data by taking subset of features_to_use\n",
    "    \n",
    "    '''\n",
    "    if features_to_use == []:\n",
    "        features_to_use = dataset_input.columns\n",
    "    \n",
    "    # Required features that must always be present\n",
    "    if 'id' not in features_to_use:\n",
    "        features_to_use.append('id')\n",
    "        \n",
    "    if 'label' not in features_to_use:\n",
    "        features_to_use.append('label')\n",
    "    \n",
    "    # Doing a copy so that the input dataset remains intact\n",
    "    dataset = dataset_input.copy(deep=True)\n",
    "    dataset = dataset[features_to_use]\n",
    "    \n",
    "    processed_data = pd.concat([dataset, vectorized_data], axis=1)\n",
    "        \n",
    "    return processed_data\n",
    "\n",
    "def preparing_data_for_training(dataset, random_state=42):\n",
    "    '''\n",
    "        Takes in the dataset as input which is the output of the preprocessing() function call\n",
    "        Applies get_dummies on the categorical columns\n",
    "        Removes webpageDescription & id from the data because they are not required for training\n",
    "        Applies train_test_split with test_size = 0.3\n",
    "        Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "        \n",
    "        Returns\n",
    "        -----------------------------\n",
    "        X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    cur_dataset = train_data.copy(deep=True)\n",
    "    \n",
    "    temp_numeric_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "    numeric_features = []\n",
    "    \n",
    "    temp_cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "    cat_features = []\n",
    "    \n",
    "    # Only consider those numeric and categorical features which are actually present in the dataset, i.e. being used for training\n",
    "    for feature in cur_dataset.columns:\n",
    "        if feature in temp_cat_features:\n",
    "            cat_features.append(feature)\n",
    "        elif feature in temp_numeric_features:\n",
    "            numeric_features.append(feature)\n",
    "    \n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    X = cur_dataset.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y = cur_dataset['label']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Standard Scaler requires us to save the changes in a copy instead of the original dataframe so that's why these copies are made\n",
    "    X_train_copy = X_train.copy(deep=True)\n",
    "    X_test_copy = X_test.copy(deep=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # Feature Standardization\n",
    "    for feature in numeric_features:\n",
    "        scaler.fit(X_train_copy[[feature]])\n",
    "        X_train_copy[feature] = scaler.transform(X_train_copy[[feature]])\n",
    "        X_test_copy[feature] = scaler.transform(X_test_copy[[feature]])\n",
    "    \n",
    "    return X_train_copy, X_test_copy, y_train, y_test\n",
    "\n",
    "def preparing_data_for_final_submission(dataset):        \n",
    "    '''\n",
    "        Apply get_dummies and feature standardization on the entire data (train.csv + test.csv)\n",
    "        Separates out train.csv and test.csv data from this processed data\n",
    "        Returns X_train (that has been processed from train.csv), y_train (from train.csv) & X_test (that has been processed from test.csv)\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        X_train, y_train, X_test\n",
    "    '''\n",
    "    cur_dataset = dataset.copy(deep=True)\n",
    "    \n",
    "    temp_numeric_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "\n",
    "    temp_cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "\n",
    "    numeric_features = []\n",
    "    cat_features = []\n",
    "    \n",
    "    # Only consider those numeric and categorical features which are actually present in the dataset\n",
    "    for feature in cur_dataset.columns:\n",
    "        if feature in temp_cat_features:\n",
    "            cat_features.append(feature)\n",
    "        elif feature in temp_numeric_features:\n",
    "            numeric_features.append(feature)\n",
    "    \n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Feature Standardization\n",
    "    for feature in numeric_features:\n",
    "        cur_dataset[feature] = scaler.fit_transform(cur_dataset[[feature]])\n",
    "    \n",
    "    train_data = cur_dataset[cur_dataset['label'].isna() == False]\n",
    "    test_data = cur_dataset[cur_dataset['label'].isna() == True]\n",
    "    \n",
    "    X_train = train_data.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    # Do not drop \"id\" from X_test\n",
    "    X_test = test_data.drop(['label', 'webpageDescription'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def generate_csv_submission(test, y_final_pred, output_file_name='submission.csv'):\n",
    "    '''\n",
    "        Parameters\n",
    "        -----------------------\n",
    "        test: Test data that contains id column\n",
    "        \n",
    "        y_final_pred: predict_proba() output for given model and test data\n",
    "        \n",
    "        output_file_name: Name of submission output file\n",
    "    '''\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"id\"] = test[\"id\"]\n",
    "    submission_df[\"label\"] = y_final_pred\n",
    "    submission_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "def end_to_end_run(data, model, vectorized_data=None, max_words_in_vocab=None, vocabulary=None, features_to_use=[], lemmatize=True, vectorizer_name='tfidf', output_file_name='submission.csv'):\n",
    "    '''\n",
    "        Utility function that does the entire modelling from start to finish\n",
    "        Vectorizes the data, fit it on the given model and generate submission file\n",
    "        \n",
    "        Parameters\n",
    "        --------------------\n",
    "        data: Input dataset\n",
    "        \n",
    "        model: Model to train data on\n",
    "        \n",
    "        vectorized_data: Output of generate_vectorized_data(), if None => this function will call vectorized_data()\n",
    "        If not None => max_words_in_vocab and vocabulary are ignored\n",
    "        \n",
    "        max_words_in_vocab: Used as max_features parameter value for vectorizing\n",
    "        \n",
    "        vocabulary: Used as vocabulary parameter value for vectorizing\n",
    "        \n",
    "        features_to_use: Which features to use for training\n",
    "    '''\n",
    "    if vectorized_data == None:\n",
    "        vectorized_data, _ = generate_vectorized_data(data, vectorizer_name, lemmatize, max_words_in_vocab, vocabulary)\n",
    "    \n",
    "    processed_data = preprocessing(data, vectorized_data, features_to_use)\n",
    "    \n",
    "    X_train_final, y_train_final, test = preparing_data_for_final_submission(processed_data)\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Drop 'id' before sending for training\n",
    "    y_final_pred = model.predict_proba(test.drop('id', axis=1))[:, 1]\n",
    "    \n",
    "    generate_csv_submission(test, y_final_pred, output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a1d32",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7ee4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_body_key(x):\n",
    "    if x['body'] == None:\n",
    "        return x['title']\n",
    "    \n",
    "    return x['body']\n",
    "\n",
    "# Generate the websiteName feature\n",
    "def generate_website_name(urls):\n",
    "    websites = urls.apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "\n",
    "    # Only retain those website_names with atleast 30 entries, assign all other website names to 'other' general category\n",
    "    websites_counts = websites.value_counts()\n",
    "    websites_with_atleast_30 = list(websites_counts[websites_counts > 30].index)\n",
    "    websites = websites.apply(lambda x: x if x in websites_with_atleast_30 else 'other')\n",
    "\n",
    "    return websites\n",
    "\n",
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Convert webpageDescription from string to JSON\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: json.loads(x))\n",
    "\n",
    "# Fill webpageDescription with 'body' key data, if None fill it with 'title' key data\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_body_key(x))\n",
    "\n",
    "# Replace all ? values in isNews and isFrontPageNews by new category 'unknown'\n",
    "merged_data['isNews'] = merged_data['isNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "merged_data['isFrontPageNews'] = merged_data['isFrontPageNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "# Assign all ? values in alchemy_category to \"unknown\" category\n",
    "merged_data['alchemy_category'] = merged_data['alchemy_category'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "# For all ? alchemy_category values we assigned them to \"unknown\" category\n",
    "# and we are 100% confident of this assignment\n",
    "# So we substitute alchemy_category_score = 1.0 (100%) for all ? values which correspond to 'unknown' category\n",
    "merged_data['alchemy_category_score'] = merged_data['alchemy_category_score'].apply(lambda x: 1.0 if x == '?' else float(x))\n",
    "\n",
    "merged_data['websiteName'] = generate_website_name(merged_data['url'])\n",
    "\n",
    "merged_data.drop(['framebased', 'embedRatio', 'AvglinkWithTwoCommonWord', 'AvglinkWithThreeCommonWord', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb437229",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_vectorized_data = pd.read_csv('tf-idf-vectorized-data-stemming-305-words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221e5944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['webpageDescription', 'alchemy_category', 'alchemy_category_score',\n",
       "       'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
       "       'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
       "       'domainLink', 'tagRatio', 'imageTagRatio', 'isNews', 'lengthyDomain',\n",
       "       'hyperlinkToAllWordsRatio', 'isFrontPageNews', 'alphanumCharCount',\n",
       "       'linksCount', 'wordCount', 'parametrizedLinkRatio',\n",
       "       'spellingErrorsRatio', 'label', 'id', 'websiteName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77114141",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'webpageDescription',\n",
    "    'avgLinkWordLength',\n",
    "    'AvglinkWithOneCommonWord',\n",
    "    'AvglinkWithFourCommonWord',\n",
    "    'redundancyMeasure',\n",
    "    'frameTagRatio',\n",
    "    'isNews',\n",
    "    'lengthyDomain',\n",
    "    'hyperlinkToAllWordsRatio',\n",
    "    'frameTagRatio',\n",
    "    'hyperlinkToAllWordsRatio',\n",
    "    'alphanumCharCount',\n",
    "    'linksCount',\n",
    "    'wordCount',\n",
    "    'spellingErrorsRatio',\n",
    "    'websiteName'\n",
    "]\n",
    "\n",
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5effcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f38df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513472335433347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4de6f7b4",
   "metadata": {},
   "source": [
    "webpageDescription only => 0.8503\n",
    "\n",
    "webpageDescription + websiteName => 0.8487\n",
    "\n",
    "webpageDescription + alchemy_category_score + alchemy_category => 0.8486\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord => 0.8497\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord => 0.8507\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure => 0.8505\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio => 0.8517\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews => 0.8518\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + isFrontPageNews => 0.8508\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain => 0.8518\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio => 0.8529\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio + alphanumCharCount => 0.8529\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio + alphanumCharCount + linksCount => 0.8531\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio + alphanumCharCount + linksCount + wordCount => 0.8531\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio + alphanumCharCount + linksCount + wordCount + parameterizedLinkRatio => 0.8528\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio + alphanumCharCount + linksCount + wordCount + spellingErrorsRatio => 0.8531\n",
    "\n",
    "webpageDescription + avgLinkWordLength + AvglinkWithOneCommonWord + AvglinkWithFourCommonWord + redundancyMeasure + frameTagRatio + isNews + lengthyDomain + hyperlinkToAllWordsRatio + alphanumCharCount + linksCount + wordCount + spellingErrorsRatio + websiteName => 0.8513\n",
    "\n",
    "webpageDescription + frameTagRatio + hyperlinkToAllWordsRatio => 0.852\n",
    "\n",
    "All columns => 0.847"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca35cd9",
   "metadata": {},
   "source": [
    "### Tentative list of features selected for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737f1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'webpageDescription',\n",
    "    'avgLinkWordLength',\n",
    "    'AvglinkWithOneCommonWord',\n",
    "    'AvglinkWithFourCommonWord',\n",
    "    'redundancyMeasure',\n",
    "    'frameTagRatio',\n",
    "    'isNews',\n",
    "    'lengthyDomain',\n",
    "    'hyperlinkToAllWordsRatio',\n",
    "    'alphanumCharCount',\n",
    "    'linksCount',\n",
    "    'wordCount'\n",
    "]\n",
    "\n",
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816276c9",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e8fe36c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.615848211066026, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "ROC AUC Score of Best Logistic Regression Hyperparameter Model: 0.8536173811029261\n"
     ]
    }
   ],
   "source": [
    "lr_param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],\n",
    "    'C' : np.logspace(-4, 4, 20)\n",
    "}\n",
    "\n",
    "lr_cv_obj = GridSearchCV(LogisticRegression(max_iter=5000), lr_param_grid, n_jobs=-1, cv=5, verbose=0, scoring='roc_auc')\n",
    "\n",
    "lr_cv_obj.fit(X_train, y_train)\n",
    "\n",
    "print(lr_cv_obj.best_params_)\n",
    "\n",
    "print(\"ROC AUC Score of Best Logistic Regression Hyperparameter Model:\", roc_auc_score(y_test, lr_cv_obj.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905dcd0",
   "metadata": {},
   "source": [
    "    Slightly better than what we obtained with default parameters 0.8531"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f2227",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Uses only webpageDescription as the model throws error for negative values in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "194b692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.02040816326530612}\n",
      "ROC AUC Score of Best Logistic Regression Hyperparameter Model: 0.8510006170535314\n"
     ]
    }
   ],
   "source": [
    "nb_param_grid = {\n",
    "    'alpha': np.linspace(0, 1, num=50)\n",
    "}\n",
    "\n",
    "\n",
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=['webpageDescription'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "nb_cv_obj = GridSearchCV(MultinomialNB(), nb_param_grid, n_jobs=-1, cv=5, verbose=0, scoring='roc_auc')\n",
    "\n",
    "nb_cv_obj.fit(X_train, y_train)\n",
    "\n",
    "print(nb_cv_obj.best_params_)\n",
    "\n",
    "print(\"ROC AUC Score of Best Multinomial Naive Bayes Hyperparameter Model:\", roc_auc_score(y_test, nb_cv_obj.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6e3ef",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "RandomizedSearch with 50 iterations because GridSearch is more time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9c84dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "{'n_estimators': 1200, 'min_samples_split': 27, 'min_samples_leaf': 3}\n",
      "ROC AUC Score of Best Random Forest Hyperparameter Model: 0.8542534794963015\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)], \n",
    "    'min_samples_split': list(range(2,40)), \n",
    "    'min_samples_leaf': list(range(1,25))\n",
    "}\n",
    "\n",
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "rf_cv_obj = RandomizedSearchCV(RandomForestClassifier(), rf_param_grid, n_jobs=-1, cv=5, n_iter=50, verbose=5, scoring='roc_auc')\n",
    "\n",
    "rf_cv_obj.fit(X_train, y_train)\n",
    "\n",
    "print(rf_cv_obj.best_params_)\n",
    "\n",
    "print(\"ROC AUC Score of Best Random Forest Hyperparameter Model:\", roc_auc_score(y_test, rf_cv_obj.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd3f09",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c38cfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "{'shrinking': False, 'gamma': 'auto', 'C': 21.54434690031882}\n",
      "ROC AUC Score of Best SVM Hyperparameter Model: 0.8463714459113219\n"
     ]
    }
   ],
   "source": [
    "svm_param_grid = {\n",
    "    'gamma': ['scale', 'auto'], \n",
    "    'shrinking': [True, False],\n",
    "    'C': np.logspace(-3, 5, 25)\n",
    "}\n",
    "\n",
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "svm_cv_obj = RandomizedSearchCV(SVC(probability=True), svm_param_grid, n_jobs=-1, cv=5, n_iter=50, verbose=5, scoring='roc_auc')\n",
    "\n",
    "svm_cv_obj.fit(X_train, y_train)\n",
    "\n",
    "print(svm_cv_obj.best_params_)\n",
    "\n",
    "print(\"ROC AUC Score of Best SVM Hyperparameter Model:\", roc_auc_score(y_test, svm_cv_obj.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe1886",
   "metadata": {},
   "source": [
    "### Training these tuned models for final submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c407b",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "059e43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "lr_model = LogisticRegression(max_iter=5000, solver='liblinear', penalty='l2', C=0.615848211066026)\n",
    "lr_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = lr_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'lr_tuned_305_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec7bb6",
   "metadata": {},
   "source": [
    "    Score of 0.85241 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cf053",
   "metadata": {},
   "source": [
    "### 2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "343b63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=['webpageDescription'])\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "nb_model = MultinomialNB(alpha=0.02040816326530612)\n",
    "\n",
    "nb_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = nb_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'nb_tuned_305_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140a5c3",
   "metadata": {},
   "source": [
    "    Score of 0.84355 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c936dab",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70b3ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=1200, min_samples_split=27, min_samples_leaf=3)\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = rf_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'rf_tuned_305_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ea6e6",
   "metadata": {},
   "source": [
    "    Score of 0.85201 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ff93a",
   "metadata": {},
   "source": [
    "### 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e7d0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=features)\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "svm_model = SVC(gamma='auto', shrinking=False, C=21.54434690031882, probability=True)\n",
    "svm_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = svm_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'svm_tuned_305_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61edb90",
   "metadata": {},
   "source": [
    "    Score of 0.84953 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbbce4",
   "metadata": {},
   "source": [
    "### Tuned Random Forest on just webpageDescription column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5af2e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=['webpageDescription'])\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=1200, min_samples_split=27, min_samples_leaf=3)\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = rf_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'rf_tuned_305_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee562190",
   "metadata": {},
   "source": [
    "    Score of 0.84584 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbfa14",
   "metadata": {},
   "source": [
    "### Repeating same experiments but with 10000 words vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e8a681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:28<00:00, 260.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "vectorized_data, vectorizer = generate_vectorized_data(merged_data, max_words_in_vocab=10000)\n",
    "\n",
    "processed_data = preprocessing(merged_data, vectorized_data, features_to_use=['webpageDescription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35021b9",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07540cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8563268301477628\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=5000, solver='liblinear', penalty='l2', C=0.615848211066026)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "lr_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = lr_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'lr_tuned_10000_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d12275",
   "metadata": {},
   "source": [
    "    Score of 0.86425 which is better than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d835a",
   "metadata": {},
   "source": [
    "### 2. Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f90d450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8574136569436298\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "nb_model = MultinomialNB(alpha=0.02040816326530612)\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "nb_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = nb_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'nb_tuned_10000_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6bdc7",
   "metadata": {},
   "source": [
    "    Score of 0.85601 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff97af",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63732e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8520290396056951\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=1200, min_samples_split=27, min_samples_leaf=3)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = rf_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'rf_tuned_10000_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db12ced",
   "metadata": {},
   "source": [
    "    Score of 0.85874 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731c233",
   "metadata": {},
   "source": [
    "### 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19bde3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8544071080503902\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "svm_model = SVC(gamma='auto', shrinking=False, C=21.54434690031882, probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "svm_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = svm_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'svm_tuned_10000_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564a622",
   "metadata": {},
   "source": [
    "    Score of 0.86206 which is worse than when we submitted with default parameters and webpageDescription only\n",
    "    for Logistic Regression (0.86312)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8542ab",
   "metadata": {},
   "source": [
    "### Trying out MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842e8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8117199541907585\n"
     ]
    }
   ],
   "source": [
    "processed_data = preprocessing(merged_data, chi2_vectorized_data, features_to_use=['webpageDescription'])\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=42)\n",
    "\n",
    "mlp_model = MLPClassifier(max_iter=5000)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(processed_data)\n",
    "\n",
    "mlp_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = mlp_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'mlp_tuned_10000_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
