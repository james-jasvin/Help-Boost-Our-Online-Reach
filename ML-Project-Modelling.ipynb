{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1858411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a81d190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d920a65",
   "metadata": {},
   "source": [
    "### NLP Processing\n",
    "\n",
    "Reference: https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "edf53b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3345346022.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3997/3345346022.py\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    def nlp_preprocessing(webpage_description, vectorizer_name='tfidf', max_words_in_vocab=None):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def preprocess_webpage_description(description):\n",
    "    # Function to convert a raw webpage description to a string of words\n",
    "    # The input is a single string (webpage description), and \n",
    "    # the output is a single string (a preprocessed webpage description)\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    description_text = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", description_text) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Stem the words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in meaningful_words]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space and return the result.\n",
    "    return \" \".join(stemmed_words\n",
    "\n",
    "    \n",
    "def nlp_preprocessing(webpage_description, vectorizer_name='tfidf', max_words_in_vocab=None):\n",
    "    \n",
    "    print(\"Cleaning webpage description...\")\n",
    "    # Preprocess each description in the column according to the function described above\n",
    "    cleaned_webpage_description = webpage_description.progress_apply(lambda x: preprocess_webpage_description(x))\n",
    "    \n",
    "    # Initialize vectorizer according to input parameters\n",
    "    if vectorizer_name == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_words_in_vocab)\n",
    "    elif vectorizer_name == \"count\":\n",
    "        vectorizer = CountVectorizer(max_features=max_words_in_vocab)\n",
    "\n",
    "    print(\"Applying vectorizer...\")\n",
    "    # Apply vectorizer to the data\n",
    "    vectorized_webpage_description = vectorizer.fit_transform(cleaned_webpage_description)\n",
    "    \n",
    "    # Converting data to a DataFrame so that it can be processed later more easily\n",
    "    vectorized_webpage_description = pd.DataFrame(vectorized_webpage_description.toarray())\n",
    "    print(\"Finished vectorization\")\n",
    "    \n",
    "    return vectorized_webpage_description\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b172e29",
   "metadata": {},
   "source": [
    "### General Preprocessing\n",
    "\n",
    "- Does data format fixing\n",
    "- Fills in missing values\n",
    "- Generates websiteName feature\n",
    "- Drops redundant or invalid columns\n",
    "- Does NLP preprocessing\n",
    "\n",
    "\n",
    "### NOTE: For all functions that follow from here on, the features_to_use parameter is considered to determine which features to select and process for training\n",
    "### features_to_use = [] => Use all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39b1d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing this function separately as its very time consuming\n",
    "def generate_vectorized_data(dataset_input, max_words_in_vocab=None, vectorizer_name='tfidf'):\n",
    "    \n",
    "    # Making a copy so that original dataset remains intact\n",
    "    dataset = dataset_input.copy(deep=True)\n",
    "    \n",
    "    # Convert webpageDescription from string to JSON\n",
    "    dataset['webpageDescription'] = dataset['webpageDescription'].apply(lambda x: json.loads(x))\n",
    "    \n",
    "    # Replace webpageDescription by its \"body\" content, if there's no \"body\" content, then replace by \"title\" content\n",
    "    dataset['webpageDescription'] = dataset['webpageDescription'].apply(lambda x: x['title'] if x['body'] == None else x['body'])\n",
    "    \n",
    "    # Vectorize the webpageDescription data\n",
    "    # Specify the name of vectorizer as \"tfidf\" or \"count\" for CountVectorizer\n",
    "    # Can also pass in the maximum words to be retained in vocabulary, otherwise vectorizer will consider all the words in the vocabulary\n",
    "    # max_words_in_vocab=5000 will consider the 5000 most frequently occurring words in the dataset\n",
    "    return nlp_preprocessing(dataset['webpageDescription'], vectorizer_name=vectorizer_name, max_words_in_vocab=max_words_in_vocab)\n",
    "\n",
    "\n",
    "# Generate the websiteName feature\n",
    "def generate_website_name(dataset):\n",
    "    dataset['websiteName'] = dataset['url'].apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "\n",
    "    # Only retain those website_names with atleast 30 entries, assign all other website names to 'other' general category\n",
    "    website_names = dataset['websiteName'].value_counts()\n",
    "    websitesWithAtleast30Entries = list(website_names[website_names > 30].index)\n",
    "    dataset['websiteName'] = dataset['websiteName'].apply(lambda x: x if x in websitesWithAtleast30Entries else 'other')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocessing(dataset_input, vectorized_data, features_to_use=[]):\n",
    "    if features_to_use == []:\n",
    "        features_to_use = dataset_input.columns\n",
    "    \n",
    "    # Required features that must always be present\n",
    "    if 'id' not in features_to_use:\n",
    "        features_to_use.append('id')\n",
    "        \n",
    "    if 'label' not in features_to_use:\n",
    "        features_to_use.append('label')\n",
    "    \n",
    "    # Columns to drop\n",
    "    # framebased because its all 0s\n",
    "    # url because after generating websiteName feature we can drop it\n",
    "    # others because they are highly correlated with other features in the dataset\n",
    "    temp_features_to_drop = ['framebased', 'embedRatio', 'AvglinkWithTwoCommonWord', 'AvglinkWithThreeCommonWord', 'url']\n",
    "    features_to_drop = []\n",
    "    \n",
    "    for feature in temp_features_to_drop:\n",
    "        if feature in features_to_use:\n",
    "            features_to_drop.append(feature)\n",
    "    \n",
    "    # Doing a copy so that the input dataset remains intact\n",
    "    dataset = dataset_input.copy(deep=True)\n",
    "    dataset = dataset[features_to_use]\n",
    "    \n",
    "    if 'url' in features_to_use:\n",
    "        dataset = generate_website_name(dataset)\n",
    "    \n",
    "    # Drop the following columns,\n",
    "    dataset.drop(features_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    processed_data = pd.concat([dataset, vectorized_data], axis=1)\n",
    "    \n",
    "    # CODE THAT REPLACES THE ? VALUES\n",
    "    \n",
    "    # Replace all ? values in isNews and isFrontPageNews by new category 'unknown'\n",
    "    if 'isNews' in features_to_use:\n",
    "        processed_data['isNews'] = processed_data['isNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "        \n",
    "    if 'isFrontPageNews' in features_to_use:\n",
    "        processed_data['isFrontPageNews'] = processed_data['isFrontPageNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "    # Assign all ? values in alchemy_category to \"unknown\" category\n",
    "    if 'alchemy_category' in features_to_use:\n",
    "        processed_data['alchemy_category'] = processed_data['alchemy_category'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "    # For all ? alchemy_category values we assigned them to \"unknown\" category\n",
    "    # and we are 100% confident of this assignment\n",
    "    # So we substitute alchemy_category_score = 1.0 (100%) for all ? values which correspond to 'unknown' category\n",
    "    if 'alchemy_category_score' in features_to_use:\n",
    "        processed_data['alchemy_category_score'] = processed_data['alchemy_category_score'].apply(lambda x: 1.0 if x == '?' else float(x))\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfe8ac",
   "metadata": {},
   "source": [
    "### Train test split for regular training\n",
    "\n",
    "This function does the following,\n",
    "- Takes in the combined processed dataset as input\n",
    "- Applies get_dummies on the categorical columns\n",
    "- Removes webpageDescription & id from the data because they are not required for training\n",
    "- Applies train_test_split with test_size = 0.3\n",
    "- Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "- Returns X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22b0ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_training(dataset, features_to_use=[], random_state=42):\n",
    "    if features_to_use == []:\n",
    "        features_to_use = dataset.columns\n",
    "        \n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    cur_dataset = train_data.copy(deep=True)\n",
    "    \n",
    "    temp_numerical_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "    numerical_features = []\n",
    "    \n",
    "    temp_cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "    cat_features = []\n",
    "    \n",
    "    # Only consider those numeric and categorical features which are specified in features_to_use\n",
    "    for feature in features_to_use:\n",
    "        if feature in temp_cat_features:\n",
    "            cat_features.append(feature)\n",
    "        elif feature in temp_numerical_features:\n",
    "            numerical_features.append(feature)\n",
    "    \n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    X = cur_dataset.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y = cur_dataset['label']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Standard Scaler requires us to save the changes in a copy instead of the original dataframe so that's why these copies are made\n",
    "    X_train_copy = X_train.copy(deep=True)\n",
    "    X_test_copy = X_test.copy(deep=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # Feature Standardization\n",
    "    for feature in numerical_features:\n",
    "        scaler.fit(X_train_copy[[feature]])\n",
    "        X_train_copy[feature] = scaler.transform(X_train_copy[[feature]])\n",
    "        X_test_copy[feature] = scaler.transform(X_test_copy[[feature]])\n",
    "    \n",
    "    return X_train_copy, X_test_copy, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07341cc1",
   "metadata": {},
   "source": [
    "### Train-test Split for final submission\n",
    "\n",
    "Very similar to the above function with the few changes being,\n",
    "- There is no actual train_test_split() call here as we use the full train.csv data\n",
    "- Apply get_dummies and feature standardization on the entire data (train.csv + test.csv)\n",
    "- Separates out train.csv and test.csv data from this processed data\n",
    "- Returns X_train (that has been processed from train.csv), y (from train.csv) & X_test (that has been processed from test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "605a4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_final_submission(dataset, features_to_use=[]):    \n",
    "    if features_to_use == []:\n",
    "        features_to_use = dataset.columns\n",
    "    \n",
    "    cur_dataset = dataset.copy(deep=True)\n",
    "    \n",
    "    temp_numerical_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "\n",
    "    temp_cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "\n",
    "    numerical_features = []\n",
    "    cat_features = []\n",
    "    \n",
    "    # Only consider those numeric and categorical features which are specified in features_to_use\n",
    "    for feature in features_to_use:\n",
    "        if feature == 'url':\n",
    "            cat_features.append('websiteName')\n",
    "        elif feature in temp_cat_features:\n",
    "            cat_features.append(feature)\n",
    "        elif feature in temp_numerical_features:\n",
    "            numerical_features.append(feature)\n",
    "    \n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Feature Standardization\n",
    "    for feature in numerical_features:\n",
    "        cur_dataset[feature] = scaler.fit_transform(cur_dataset[[feature]])\n",
    "    \n",
    "    train_data = cur_dataset[cur_dataset['label'].isna() == False]\n",
    "    test_data = cur_dataset[cur_dataset['label'].isna() == True]\n",
    "    \n",
    "    X_train = train_data.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    # Do not drop \"id\" from X_test\n",
    "    X_test = test_data.drop(['label', 'webpageDescription'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d8323",
   "metadata": {},
   "source": [
    "### Function that runs end-to-end to generate submission file\n",
    "\n",
    "- If vectorized_data == None, then first calls nlp_preprocessing() using the given parameters (max_words_in_vocab, vectorizer_name) in function call.\n",
    "- General preprocessing by calling preprocessing()\n",
    "- Calls preparing_data_for_final_submission()\n",
    "- Trains on the given model in function call\n",
    "- Prepares the csv file to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50ebdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that the function can be called separately when need be\n",
    "def generate_csv_submission(test, y_final_pred, output_file_name='submission.csv'):\n",
    "    # Preparing file to be submitted\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"id\"] = test[\"id\"]\n",
    "    submission_df[\"label\"] = y_final_pred\n",
    "    submission_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# features_to_use = None => Use all features\n",
    "# vectorized_data = None => Vectorize the data using the vectorizer specified in the parameters\n",
    "# If vectorized_data != None => max_words_in_vocab, vectorizer_name parameters are ignored\n",
    "def end_to_end_run(data, model, vectorized_data=None, max_words_in_vocab=None, features_to_use=[], vectorizer_name='tfidf', output_file_name='submission.csv'):\n",
    "    if features_to_use == []:\n",
    "        features_to_use = list(data.columns)\n",
    "    \n",
    "    if vectorized_data == None:\n",
    "        vectorized_data = generate_vectorized_data(data, max_words_in_vocab, vectorizer_name)\n",
    "    \n",
    "    processed_data = preprocessing(data, vectorized_data, features_to_use)\n",
    "    \n",
    "    X_train_final, y_train_final, test = preparing_data_for_final_submission(processed_data, features_to_use)\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Drop 'id' before sending for training\n",
    "    y_final_pred = model.predict(test.drop('id', axis=1))\n",
    "    \n",
    "    generate_csv_submission(test, y_final_pred, output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551aef80",
   "metadata": {},
   "source": [
    "### Example use-case of preparing_data_for_training() for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0fcaacbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:28<00:00, 259.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "vectorized_data = generate_vectorized_data(merged_data, max_words_in_vocab=10000, vectorizer_name='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db0dad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(merged_data, vectorized_data, features_to_use=[])\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb531b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7978287420618698\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33640a4",
   "metadata": {},
   "source": [
    "### Example use-case of creating final submission using preparing_data_for_final_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7322e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, y_train_final, test = preparing_data_for_final_submission(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f8acb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Drop 'id' before sending for training\n",
    "y_final_pred = model.predict(test.drop('id', axis=1))\n",
    "\n",
    "generate_csv_submission(test, y_final_pred, 'test_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64dc27",
   "metadata": {},
   "source": [
    "### Example use-case of end_to_end_run() to create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00b77eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:27<00:00, 268.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "\n",
    "end_to_end_run(merged_data, model, max_words_in_vocab=10000, features_to_use=[], vectorizer_name='tfidf', output_file_name='test_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b4ee6",
   "metadata": {},
   "source": [
    "### Using only the webpageDescription column for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "60ca19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:28<00:00, 262.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "end_to_end_run(data=merged_data, max_words_in_vocab=10000, model=model, features_to_use=['webpageDescription'], vectorizer_name='tfidf', output_file_name='Only_Description+10000_words_TFIDF.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
