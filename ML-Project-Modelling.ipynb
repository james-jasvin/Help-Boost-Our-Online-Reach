{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1858411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81d190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Convert webpageDescription from string to JSON\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eeee37",
   "metadata": {},
   "source": [
    "### Fill webpageDescription using either title or body key\n",
    "\n",
    "There are two approaches to use here for which there are two separate functions,\n",
    "\n",
    "    - Fill all entries with body key and whereever body is None, fill it with title key => use_title_key()\n",
    "    - Fill all entries with title key and whereever title is None, fill it with body key => use_body_key()\n",
    "\n",
    "That's what the two functions listed here do and lambda function will apply it on the webpageDescription column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b603f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def use_title_key(x):\n",
    "    # Some entries don't have title key, in that case add 'title' key with value as None\n",
    "    # to avoid KeyError in the next if condition\n",
    "    x.setdefault('title', None)\n",
    "    \n",
    "    if x['title'] == None:\n",
    "        return x['body']\n",
    "    \n",
    "    return x['title']\n",
    "\n",
    "def use_body_key(x):\n",
    "    if x['body'] == None:\n",
    "        return x['title']\n",
    "    \n",
    "    return x['body']\n",
    "\n",
    "# COMMENT OR UNCOMMENT THESE LINES DEPENDING ON WHICH DATA YOU WANT IN THE webpageDescription column\n",
    "# merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_title_key(x))\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_body_key(x))\n",
    "print(merged_data['webpageDescription'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2ea5f",
   "metadata": {},
   "source": [
    "### Fill in ? values in the columns \n",
    "\n",
    "This can be changed as and when we determine new logic to fill in ? values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304eb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all ? values in isNews and isFrontPageNews by new category 'unknown'\n",
    "merged_data['isNews'] = merged_data['isNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "        \n",
    "merged_data['isFrontPageNews'] = merged_data['isFrontPageNews'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "# Assign all ? values in alchemy_category to \"unknown\" category\n",
    "merged_data['alchemy_category'] = merged_data['alchemy_category'].apply(lambda x: 'unknown' if x == '?' else x)\n",
    "    \n",
    "# For all ? alchemy_category values we assigned them to \"unknown\" category\n",
    "# and we are 100% confident of this assignment\n",
    "# So we substitute alchemy_category_score = 1.0 (100%) for all ? values which correspond to 'unknown' category\n",
    "merged_data['alchemy_category_score'] = merged_data['alchemy_category_score'].apply(lambda x: 1.0 if x == '?' else float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a926d7",
   "metadata": {},
   "source": [
    "### Adding website name feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d36fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the websiteName feature\n",
    "def generate_website_name(urls):\n",
    "    websites = urls.apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "\n",
    "    # Only retain those website_names with atleast 30 entries, assign all other website names to 'other' general category\n",
    "    websites_counts = websites.value_counts()\n",
    "    websites_with_atleast_30 = list(websites_counts[websites_counts > 30].index)\n",
    "    websites = websites.apply(lambda x: x if x in websites_with_atleast_30 else 'other')\n",
    "\n",
    "    return websites\n",
    "\n",
    "merged_data['websiteName'] = generate_website_name(merged_data['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2adbd9",
   "metadata": {},
   "source": [
    "### Dropping redundant columns\n",
    "\n",
    "    framebased because its all 0s\n",
    "    url because after generating websiteName feature we can drop it\n",
    "    others because they are highly correlated with other features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64057bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(['framebased', 'embedRatio', 'AvglinkWithTwoCommonWord', 'AvglinkWithThreeCommonWord', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d920a65",
   "metadata": {},
   "source": [
    "### NLP Processing\n",
    "\n",
    "Reference: https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf53b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description, lemmatize=True):\n",
    "    # Function to convert a raw webpage description to a string of words\n",
    "    # The input is a single string (webpage description), and \n",
    "    # the output is a single string (a preprocessed webpage description)\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    description_text = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", description_text) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Stem or Lemmatize the words\n",
    "    if lemmatize == False:\n",
    "        porter = PorterStemmer()\n",
    "        output_words = [porter.stem(word) for word in meaningful_words]\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        output_words = [lemmatizer.lemmatize(word) for word in meaningful_words]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space and return the result.\n",
    "    return \" \".join(output_words)\n",
    "\n",
    "\n",
    "# Vectorize the webpageDescription data\n",
    "# Specify the name of vectorizer as \"tfidf\" or \"count\" for CountVectorizer\n",
    "# Can also pass in the maximum words to be retained in vocabulary, otherwise vectorizer will consider all the words in the vocabulary\n",
    "# max_words_in_vocab=5000 will consider the 5000 most frequently occurring words in the dataset\n",
    "def generate_vectorized_data(webpage_description, vectorizer_name='tfidf', lemmatize=True, max_words_in_vocab=None):\n",
    "    \n",
    "    print(\"Cleaning webpage description...\")\n",
    "    # Preprocess each description in the column according to the function described above\n",
    "    cleaned_webpage_description = webpage_description.progress_apply(lambda x: preprocess_webpage_description(x, lemmatize))\n",
    "    \n",
    "    # Initialize vectorizer according to input parameters\n",
    "    if vectorizer_name == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_words_in_vocab)\n",
    "    elif vectorizer_name == \"count\":\n",
    "        vectorizer = CountVectorizer(max_features=max_words_in_vocab)\n",
    "\n",
    "    print(\"Applying vectorizer...\")\n",
    "    # Apply vectorizer to the data\n",
    "    vectorized_webpage_description = vectorizer.fit_transform(cleaned_webpage_description)\n",
    "    \n",
    "    # Converting data to a DataFrame so that it can be processed later more easily\n",
    "    vectorized_webpage_description = pd.DataFrame(vectorized_webpage_description.toarray())\n",
    "    print(\"Finished vectorization\")\n",
    "    \n",
    "    return vectorized_webpage_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b172e29",
   "metadata": {},
   "source": [
    "### General Preprocessing\n",
    "\n",
    "- Takes in features to use for training as input and selects them (id and label are always considered).\n",
    "- Also takes in vectorized_data as NLP and concatenates it with the sliced data\n",
    "\n",
    "\n",
    "### NOTE: For all functions that follow from here on, the features_to_use parameter is considered to determine which features to select and process for training\n",
    "### features_to_use = [] => Use all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b1d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset_input, vectorized_data, features_to_use=[]):\n",
    "    if features_to_use == []:\n",
    "        features_to_use = dataset_input.columns\n",
    "    \n",
    "    # Required features that must always be present\n",
    "    if 'id' not in features_to_use:\n",
    "        features_to_use.append('id')\n",
    "        \n",
    "    if 'label' not in features_to_use:\n",
    "        features_to_use.append('label')\n",
    "    \n",
    "    # Doing a copy so that the input dataset remains intact\n",
    "    dataset = dataset_input.copy(deep=True)\n",
    "    dataset = dataset[features_to_use]\n",
    "    \n",
    "    processed_data = pd.concat([dataset, vectorized_data], axis=1)\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfe8ac",
   "metadata": {},
   "source": [
    "### Train test split for regular training\n",
    "\n",
    "This function does the following,\n",
    "- Takes in the dataset as input which is the output of the preprocessing() function call\n",
    "- Applies get_dummies on the categorical columns\n",
    "- Removes webpageDescription & id from the data because they are not required for training\n",
    "- Applies train_test_split with test_size = 0.3\n",
    "- Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "- Returns X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b0ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_training(dataset, random_state=42):\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    cur_dataset = train_data.copy(deep=True)\n",
    "    \n",
    "    temp_numeric_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "    numeric_features = []\n",
    "    \n",
    "    temp_cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "    cat_features = []\n",
    "    \n",
    "    # Only consider those numeric and categorical features which are actually present in the dataset, i.e. being used for training\n",
    "    for feature in cur_dataset.columns:\n",
    "        if feature in temp_cat_features:\n",
    "            cat_features.append(feature)\n",
    "        elif feature in temp_numeric_features:\n",
    "            numeric_features.append(feature)\n",
    "    \n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    X = cur_dataset.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y = cur_dataset['label']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Standard Scaler requires us to save the changes in a copy instead of the original dataframe so that's why these copies are made\n",
    "    X_train_copy = X_train.copy(deep=True)\n",
    "    X_test_copy = X_test.copy(deep=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # Feature Standardization\n",
    "    for feature in numeric_features:\n",
    "        scaler.fit(X_train_copy[[feature]])\n",
    "        X_train_copy[feature] = scaler.transform(X_train_copy[[feature]])\n",
    "        X_test_copy[feature] = scaler.transform(X_test_copy[[feature]])\n",
    "    \n",
    "    return X_train_copy, X_test_copy, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07341cc1",
   "metadata": {},
   "source": [
    "### Train-test Split for final submission\n",
    "\n",
    "Very similar to the above function with the few changes being,\n",
    "- There is no actual train_test_split() call here as we use the full train.csv data\n",
    "- Apply get_dummies and feature standardization on the entire data (train.csv + test.csv)\n",
    "- Separates out train.csv and test.csv data from this processed data\n",
    "- Returns X_train (that has been processed from train.csv), y (from train.csv) & X_test (that has been processed from test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605a4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_final_submission(dataset):        \n",
    "    cur_dataset = dataset.copy(deep=True)\n",
    "    \n",
    "    temp_numeric_features = ['alchemy_category_score', 'avgLinkWordLength', 'AvglinkWithOneCommonWord',\n",
    "                          'AvglinkWithFourCommonWord', 'redundancyMeasure', 'frameTagRatio',\n",
    "                          'tagRatio', 'imageTagRatio', 'hyperlinkToAllWordsRatio',\n",
    "                          'alphanumCharCount', 'linksCount', 'wordCount',\n",
    "                          'parametrizedLinkRatio', 'spellingErrorsRatio'\n",
    "                         ]\n",
    "\n",
    "    temp_cat_features = ['alchemy_category', 'domainLink', 'isNews', 'isFrontPageNews', 'lengthyDomain', 'websiteName']\n",
    "\n",
    "    numeric_features = []\n",
    "    cat_features = []\n",
    "    \n",
    "    # Only consider those numeric and categorical features which are actually present in the dataset\n",
    "    for feature in cur_dataset.columns:\n",
    "        if feature in temp_cat_features:\n",
    "            cat_features.append(feature)\n",
    "        elif feature in temp_numeric_features:\n",
    "            numeric_features.append(feature)\n",
    "    \n",
    "    # Get dummies on categorical columns\n",
    "    cur_dataset = pd.get_dummies(cur_dataset, columns=cat_features, drop_first=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Feature Standardization\n",
    "    for feature in numeric_features:\n",
    "        cur_dataset[feature] = scaler.fit_transform(cur_dataset[[feature]])\n",
    "    \n",
    "    train_data = cur_dataset[cur_dataset['label'].isna() == False]\n",
    "    test_data = cur_dataset[cur_dataset['label'].isna() == True]\n",
    "    \n",
    "    X_train = train_data.drop(['label', 'webpageDescription', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    # Do not drop \"id\" from X_test\n",
    "    X_test = test_data.drop(['label', 'webpageDescription'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456afb52",
   "metadata": {},
   "source": [
    "### Function that runs end-to-end to generate submission file\n",
    "\n",
    "- If vectorized_data == None, then first calls nlp_preprocessing() using the given parameters (max_words_in_vocab, vectorizer_name) in function call.\n",
    "- General preprocessing by calling preprocessing()\n",
    "- Calls preparing_data_for_final_submission()\n",
    "- Trains on the given model in function call\n",
    "- Prepares the csv file to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e74e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that the function can be called separately when need be\n",
    "def generate_csv_submission(test, y_final_pred, output_file_name='submission.csv'):\n",
    "    # Preparing file to be submitted\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"id\"] = test[\"id\"]\n",
    "    submission_df[\"label\"] = y_final_pred\n",
    "    submission_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# features_to_use = None => Use all features\n",
    "# vectorized_data = None => Vectorize the data using the vectorizer specified in the parameters\n",
    "# If vectorized_data != None => max_words_in_vocab, vectorizer_name parameters are ignored\n",
    "def end_to_end_run(data, model, vectorized_data=None, max_words_in_vocab=None, features_to_use=[], lemmatize=True, vectorizer_name='tfidf', output_file_name='submission.csv'):\n",
    "    if vectorized_data == None:\n",
    "        vectorized_data = generate_vectorized_data(data['webpageDescription'], vectorizer_name, lemmatize, max_words_in_vocab)\n",
    "    \n",
    "    processed_data = preprocessing(data, vectorized_data, features_to_use)\n",
    "    \n",
    "    X_train_final, y_train_final, test = preparing_data_for_final_submission(processed_data)\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Drop 'id' before sending for training\n",
    "    y_final_pred = model.predict(test.drop('id', axis=1))\n",
    "    \n",
    "    generate_csv_submission(test, y_final_pred, output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551aef80",
   "metadata": {},
   "source": [
    "### Example use-case of preparing_data_for_training() for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5765a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7394/7394 [00:10<00:00, 716.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "vectorized_data = generate_vectorized_data(merged_data['webpageDescription'], lemmatize=True, max_words_in_vocab=40000, vectorizer_name='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0dad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing(merged_data, vectorized_data, features_to_use=[])\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_data_for_training(processed_data, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb531b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7961433573624966\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33640a4",
   "metadata": {},
   "source": [
    "### Example use-case of creating final submission using preparing_data_for_final_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7322e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, y_train_final, test = preparing_data_for_final_submission(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f8acb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Drop 'id' before sending for training\n",
    "y_final_pred = model.predict(test.drop('id', axis=1))\n",
    "\n",
    "generate_csv_submission(test, y_final_pred, 'test_submission_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6516c70",
   "metadata": {},
   "source": [
    "### Example use-case of end_to_end_run() to create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0562be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7394/7394 [00:07<00:00, 1043.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "end_to_end_run(merged_data, model, max_words_in_vocab=10000, features_to_use=[], vectorizer_name='tfidf', output_file_name='test_submission_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146295f",
   "metadata": {},
   "source": [
    "### Using only the webpageDescription column for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b1010da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning webpage description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7394/7394 [00:06<00:00, 1068.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying vectorizer...\n",
      "Finished vectorization\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "end_to_end_run(data=merged_data, max_words_in_vocab=10000, model=model, features_to_use=['webpageDescription'], vectorizer_name='tfidf', output_file_name='test_submission_3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
