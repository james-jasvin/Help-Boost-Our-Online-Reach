{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b156f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB # Doesn't work for Word2Vec because of negative values in word vectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Logging to display info regarding training of models especially Word2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790d3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "# Drop the only entry which has neither \"body\" nor \"title\" in its webpageDescription\n",
    "train.drop(index=2994, inplace=True)\n",
    "\n",
    "test = pd.read_csv('dataset/test_data.csv')\n",
    "\n",
    "merged_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Convert webpageDescription from string to JSON\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff38f1",
   "metadata": {},
   "source": [
    "### Filling in webpageDescription\n",
    "\n",
    "    Use the body key value if non-empty\n",
    "    Else use the title key vallue\n",
    "    Else use the url key value\n",
    "    Else just fill it with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6fc511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def use_body_key(x):\n",
    "    # strip() function is used to ensure that only blank descriptions don't pass through this condition\n",
    "    if x['body'] == None or len(x['body'].strip()) == 0:\n",
    "        if x['title'] == None or len(x['title'].strip()) == 0:\n",
    "            if x['url'] == None or len(x['url'].strip()) == 0:\n",
    "                return 'unknown'\n",
    "            return x['url']\n",
    "        return x['title']\n",
    "    \n",
    "    return x['body']\n",
    "\n",
    "merged_data['webpageDescription'] = merged_data['webpageDescription'].apply(lambda x: use_body_key(x))\n",
    "print(merged_data['webpageDescription'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4beb2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_for_training(dataset, random_state=42):\n",
    "    '''\n",
    "        Takes in the dataset as input which is the output of the preprocessing() function call\n",
    "        Applies get_dummies on the categorical columns\n",
    "        Removes webpageDescription & id from the data because they are not required for training\n",
    "        Applies train_test_split with test_size = 0.3\n",
    "        Applies StandardScaler by fitting on X_train and transforming both X_train & X_test\n",
    "        \n",
    "        Returns\n",
    "        -----------------------------\n",
    "        X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    \n",
    "    X = train_data.drop(['label', 'id'], axis=1)\n",
    "    y = train_data['label']\n",
    "        \n",
    "    return train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "def preparing_data_for_final_submission(dataset):        \n",
    "    '''\n",
    "        Apply get_dummies and feature standardization on the entire data (train.csv + test.csv)\n",
    "        Separates out train.csv and test.csv data from this processed data\n",
    "        Returns X_train (that has been processed from train.csv), y_train (from train.csv) & X_test (that has been processed from test.csv)\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        X_train, y_train, X_test\n",
    "    '''\n",
    "    train_data = dataset[dataset['label'].isna() == False]\n",
    "    test_data = dataset[dataset['label'].isna() == True]\n",
    "    \n",
    "    X_train = train_data.drop(['label', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    \n",
    "    # Do not drop \"id\" from X_test\n",
    "    X_test = test_data.drop(['label'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def generate_csv_submission(test, y_final_pred, output_file_name='submission.csv'):\n",
    "    '''\n",
    "        Parameters\n",
    "        -----------------------\n",
    "        test: Test data that contains id column\n",
    "        \n",
    "        y_final_pred: predict_proba() output for given model and test data\n",
    "        \n",
    "        output_file_name: Name of submission output file\n",
    "    '''\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"id\"] = test[\"id\"]\n",
    "    submission_df[\"label\"] = y_final_pred\n",
    "    submission_df.to_csv(output_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c74f0",
   "metadata": {},
   "source": [
    "word2vec requires a single sentence as input and a sentence is treated as a list of words, so this function returns a list of words\n",
    "\n",
    "Removing stopwords and numbers can be detrimental to the learning process, so they're not removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cacadc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webpage_description(description, remove_stopwords=False, no_empty_lists=False):\n",
    "    # Function to convert a raw webpage description to a string of words\n",
    "    # The input is a single string (webpage description), and \n",
    "    # the output is a single string (a preprocessed webpage description)\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    words = BeautifulSoup(description).get_text() \n",
    "\n",
    "    # 2. Remove non-alphanumeric values\n",
    "    words = re.sub(\"[^a-zA-Z\\d]\", \" \", words) \n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = words.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ac310",
   "metadata": {},
   "source": [
    "### Word2Vec approach: Trigrams data as input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0978533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7394/7394 [00:02<00:00, 2700.03it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_data = merged_data.copy(deep=True)\n",
    "tokenized_description_data = processed_data['webpageDescription'].progress_apply(lambda x: preprocess_webpage_description(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64b83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 14:12:51,138 : INFO : collecting all words and their counts\n",
      "2021-12-21 14:12:51,140 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-12-21 14:12:55,789 : INFO : collected 1126483 token types (unigram + bigrams) from a corpus of 3349471 words and 7394 sentences\n",
      "2021-12-21 14:12:55,790 : INFO : merged Phrases<1126483 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-12-21 14:12:55,791 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1126483 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 4.65s', 'datetime': '2021-12-21T14:12:55.791422', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-40-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "2021-12-21 14:12:55,819 : INFO : collecting all words and their counts\n",
      "2021-12-21 14:12:55,820 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-12-21 14:13:03,940 : INFO : collected 1277504 token types (unigram + bigrams) from a corpus of 2988442 words and 7394 sentences\n",
      "2021-12-21 14:13:03,941 : INFO : merged Phrases<1277504 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-12-21 14:13:03,942 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1277504 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 8.12s', 'datetime': '2021-12-21T14:13:03.942376', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-40-generic-x86_64-with-glibc2.29', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# tokenized_description_data => Input to Phrases() => Output = Bigrams data\n",
    "# Bigrams data => Input to Phrases() => Output = Trigrams data\n",
    "\n",
    "bigrams = Phrases(sentences=tokenized_description_data)\n",
    "trigrams = Phrases(sentences=bigrams[tokenized_description_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb1a7c",
   "metadata": {},
   "source": [
    "### Training Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff2764a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out to directly use saved model\n",
    "# num_features = 300\n",
    "# context = 10\n",
    "\n",
    "# trigrams_w2v_model = Word2Vec(\n",
    "#     sentences = trigrams[bigrams[tokenized_description_data]],\n",
    "#     vector_size=num_features,\n",
    "#     min_count=1, window=10, workers=4,\n",
    "#     sg=1\n",
    "# )\n",
    "\n",
    "# model_name = 'trigrams_300features_1minwords_10context_sg'\n",
    "\n",
    "# trigrams_w2v_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac8566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 14:13:03,977 : INFO : loading Word2Vec object from trigrams_300features_1minwords_10context_sg\n",
      "2021-12-21 14:13:04,016 : INFO : loading wv recursively from trigrams_300features_1minwords_10context_sg.wv.* with mmap=None\n",
      "2021-12-21 14:13:04,017 : INFO : loading vectors from trigrams_300features_1minwords_10context_sg.wv.vectors.npy with mmap=None\n",
      "2021-12-21 14:13:04,137 : INFO : loading syn1neg from trigrams_300features_1minwords_10context_sg.syn1neg.npy with mmap=None\n",
      "2021-12-21 14:13:04,261 : INFO : setting ignored attribute cum_table to None\n",
      "2021-12-21 14:13:05,049 : INFO : Word2Vec lifecycle event {'fname': 'trigrams_300features_1minwords_10context_sg', 'datetime': '2021-12-21T14:13:05.049554', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.11.0-40-generic-x86_64-with-glibc2.29', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "trigrams_w2v_model = Word2Vec.load('trigrams_300features_1minwords_10context_sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827c78c",
   "metadata": {},
   "source": [
    "### Averaging word vectors to get feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45bac7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    # This array will contain the sum of all word vectors for the given description\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    \n",
    "    # This counts the number of words from given description whose word vectors are used\n",
    "    # to compute the overall word embedding for this description\n",
    "    nwords = 0.\n",
    "\n",
    "    # index_to_key is a list that contains the names of the words in the model's vocabulary\n",
    "    # Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "\n",
    "    # There are outlier cases where after doing the preprocessing steps, i.e. after removing non-alphanumeric\n",
    "    # characters there are no words left, so sentence remains an empty list which is a problem for training the\n",
    "    # word2vec model, so we just return a list containing 'unknown' as the sole word\n",
    "    # Note: This typically happens with a few entries where description contains only Japanese characters and such\n",
    "    if len(words) == 0:\n",
    "        words = ['unknown']\n",
    "    \n",
    "    # Loop over each word in the description and if it is in the model's vocabulary,\n",
    "    # add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            # Add the word vector of given word in featureVec\n",
    "            featureVec = np.add(featureVec, model.wv[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    \n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(descriptions, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "     \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    descriptionFeatureVecs = np.zeros((len(descriptions),num_features),dtype=\"float32\")\n",
    "    \n",
    "    # Loop through the reviews\n",
    "    for i, description in enumerate(tqdm(descriptions)):\n",
    "         \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        descriptionFeatureVecs[i] = makeFeatureVec(description, model, num_features)\n",
    "        \n",
    "    return descriptionFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17972d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7394/7394 [01:23<00:00, 88.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# num_features = Same as that used for training\n",
    "vectorized_data = pd.DataFrame(getAvgFeatureVecs(tokenized_description_data, trigrams_w2v_model, num_features=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f30f5",
   "metadata": {},
   "source": [
    "### Concatenate label and id with vectorized data\n",
    "\n",
    "So that predictions can be made with labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77bf12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_data = pd.concat([processed_data[['label','id']], vectorized_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97728e",
   "metadata": {},
   "source": [
    "### Training vectorized data on best performing MLP architecture, hidden layers = (100, 50, 100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13fffa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50, 100, 50),\n",
    "    activation='relu',\n",
    "    learning_rate='adaptive',\n",
    "    solver='sgd',\n",
    "    max_iter=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000ca79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score of Best MLP Classifier Hyperparameter Model: 0.8701375292973462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasvin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_data_for_training(modelling_data)\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"ROC AUC Score of Best MLP Classifier Hyperparameter Model:\", roc_auc_score(y_test, mlp_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46c99da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasvin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_final, y_train_final, X_test_final = preparing_data_for_final_submission(modelling_data)\n",
    "\n",
    "mlp_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "y_final_pred = mlp_model.predict_proba(X_test_final.drop('id', axis=1))[:, 1]\n",
    "\n",
    "generate_csv_submission(X_test_final, y_final_pred, 'word2vec_mlp_2_trigrams_300features_1minwords_10context_sg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
